```{r include=FALSE}
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(tidybayes, quietly = TRUE)
library(rethinking)
library(brms)
library(loo)
library(modelr)
library(simstudy)
library(posterior)
library(dagitty, quietly = TRUE)
library(ggdag, quietly = TRUE)
library(ggdist, quietly = TRUE)
library(ggmcmc, quietly = TRUE)
library(bayesplot, quietly = TRUE)
library(patchwork, quietly = TRUE)
library(paletteer, quietly = TRUE)
```


# Monsters and Mixtures {#Mixed}


## Over-dispersed outcomes

### Beta-binomial

#### Beta-binomial distribution

The beta distribution is

$$
\mathcal{Beta}(x|\alpha, \beta) =
\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha -1} (1-x)^{\beta -1} =
\frac{1}{B(\alpha, \beta)} x^{\alpha -1} (1-x)^{\beta -1}, 0 \leq x \leq 1
$$
which is not the format used by McElreath.  He uses the following
shape parameters which are much easier to understand as $\mu$ is the **average**
of the distribution and $\kappa$ is the **spread**.

$$
\mu = \bar{p} = \frac{\alpha}{\alpha + \beta} \\
\kappa = \theta = \alpha + \beta
$$

the `simstudy` package provide the function to perform that conversion from
$mean = \mu$ and $precision = \kappa$ to the shape(mathematical) parameters $\alpha$ and $\beta$ 

```{r}
paramsMeanKappa <- list(mean = 0.5, kappa = 5)
paramsShape <- with(paramsMeanKappa, simstudy::betaGetShapes(mean, kappa))
stopifnot(paramsShape$shape1 == paramsMeanKappa$mean * paramsMeanKappa$kappa,
          paramsShape$shape2 == (1 - paramsMeanKappa$mean) * paramsMeanKappa$kappa)
```

and the parameters are used in different `beta` functions but give the same
result.

```{r}
dens1 <- with(paramsMeanKappa, rethinking::dbeta2(0.25, mean, kappa))
dens2 <- with(paramsShape, dbeta(0.25, shape1, shape2))
stopifnot(dens1 == dens2)
```


and the standard deviation of the beta binomial distribution is

$$
\sigma = \sqrt{\frac{\mu(1-\mu)}{\kappa+1}}
$$


The beta-binomial distribution is not defined in `brms`.  We need to define the 
family in ``brms` as well as a `stan_funs()` and `stanvar()`.

```{r}
# IMPORTANT: we could have used lb = c(NA, 0) as Kurtz does
#            but McElreath adds 2 to theta
# see note just below on lb = c(NA, 2)
beta_binomial2 <- custom_family(
  "beta_binomial2", dpars = c("mu", "phi"),
  links = c("logit", "log"), lb = c(NA, 2),
  type = "int", vars = "vint1[n]"
)

stan_funs <- "
  real beta_binomial2_lpmf(int y, real mu, real phi, int T) {
    return beta_binomial_lpmf(y | T, mu * phi, (1 - mu) * phi);
  }
  int beta_binomial2_rng(real mu, real phi, int T) {
    return beta_binomial_rng(T, mu * phi, (1 - mu) * phi);
  }
"

stanvars <- stanvar(scode = stan_funs, block = "functions")
```


> Did you notice `lb = c(NA, 2)`? In Burkner's vignette the lower bound of $\phi$ 
is 0.  Since McElreath wanted the lower bound to 2, we will use lb = 2.

See also McElreath explanation of 2 in section 12.1.1 just before R code 12.1
on p. 371.


Variations of the beta-binomial distribution using different parameter values
can be illustrated as follows


```{r}
p <- list()
p$df <- crossing(pbar = c(0.25, 0.5, 0.75), theta = c(5, 15, 30)) %>% 
  expand(nesting(pbar, theta), 
         x = seq(from = 0, to = 1, length.out = 100)) %>%
  mutate(shape1 = betaGetShapes(pbar, theta)$shape1,
         shape2 = betaGetShapes(pbar, theta)$shape2) %>%
  mutate(density = dbeta(x, shape1, shape2),
         mu = paste("mu", pbar, sep = "=="),
         kappa = paste("kappa", theta, sep = "=="))

p$plot <- ggplot(data = p$df, aes(x = x, y = density)) +
  geom_area(fill = "darkorchid1") + 
  scale_y_continuous(NULL, labels = NULL) +
  ggthemes::theme_hc() +
  theme(axis.ticks.y = element_blank(),
        plot.background = element_rect(fill = "grey92")) +
  labs(title = "Beta can take many shapes",
       x = "parameter space") +
  facet_grid(kappa~mu, labeller = label_parsed)
p$plot
```

#### Beta-binomial model



The data used is

```{r}
data(UCBadmit)
dataAdmit <- UCBadmit %>%
  mutate(gid = ifelse(applicant.gender == "male", "1", "2"))
rm(UCBadmit)
# glimpse(dataAdmit)
```



There is an error in the model defined by McElrath, to concur with his code
at 11.26, the model is

$$
\begin{align*}
admit_i &\sim \mathcal{BetaBinomial}(N_i, \bar{p}_i, \phi) \\
logit(\bar{p}_i) &= \alpha_{gid[i]} \\
\alpha &\sim \mathcal{N}(0, 1.5) \\
\phi &\sim \mathcal{Exponential}(1)
\end{align*}
$$


which we fit as follows, see important note above on `beta_binomial2()`

```{r}
a_file <- here::here("fits", "b12_01.rds")
b12.1 <- readRDS(file = a_file)
# b12.1 <- brm(
#   data = dataAdmit,
#   family = beta_binomial2,
#   admit | vint(applications) ~ 0 + gid,
#   prior = c(prior(normal(0, 1.5), class = b),
#             prior(exponential(1), class = phi)),
#   cores = detectCores(),
#   stanvars = stanvars,
#   seed = 12)
# NOT USED b12.1 <- add_criterion(b12.1, criterion = c("waic", "loo"))
# saveRDS(object = b12.1, file = a_file)
```

with the summary

```{r}
summary(b12.1)
```


and the posterior data which *represents the distribution rather than the data*

```{r}
samples <- list()
samples$data <- as_draws_df(b12.1) %>%
  mutate_variables(a_diff = b_gid1 - b_gid2,
                   p_gid1 = gtools::inv.logit(b_gid1),
                   p_gid2 = gtools::inv.logit(b_gid2))
samples$data %>%
  summarize_draws("mean", "median", "sd", "mad", 
                  ~quantile2(.x, probs = c(0.055, 0.945)),
                  default_convergence_measures()) %>%
  filter(variable != "lp__") %>%
  mutate(across(.cols = where(is.double), .fns = round, digits = 2))

# stats used in the plot later
samples$stats <- list(
  mean_pgid2 = mean(samples$data$p_gid2),
  mean_phi = mean(samples$data$phi)
)
```

and so, just like McElreath, the difference between the admission rates `a_diff`
is close to zero.


#### Beta-binomial plots

See @kurtz2020b for the details

```{r}
set.seed(12)

p <- list()
p$df <-
  samples$data %>%
  slice_sample(n = 100) %>%
  expand(nesting(.draw, p_gid2, phi),
         x = seq(from = 0, to = 1, by = .005)) %>%
  mutate(density = purrr::pmap_dbl(list(x, p_gid2, phi), rethinking::dbeta2))
glimpse(p$df)
samples$stats
```

```{r}
ggplot(p$df, aes(x = x, y = density)) + 
  stat_function(fun = rethinking::dbeta2,
                args = list(prob = samples$stats$mean_pgid2,
                            theta = samples$stats$mean_phi),
                size = 1, color = "violetred") +
  geom_line(aes(group = .draw),
            alpha = .2, color = "darkgreen") +
  scale_y_continuous(breaks = scales::breaks_width(width = 0.5), limits = c(0, 3)) +
  ggthemes::theme_few() +
  theme(panel.background = element_rect(fill = "wheat")) +
  labs(subtitle = "distribution of female admission rates",
       x = "probability admit", y = "density")
```


and for the posterior validation check we need to create funcitions to handle
predictions and fitted values.  See @kurtz2020b.


```{r}
expose_functions(b12.1, vectorize = TRUE)

# required to use `predict()`
log_lik_beta_binomial2 <- function(i, prep) {
  mu     <- prep$dpars$mu[, i]
  phi    <- prep$dpars$phi
  trials <- prep$data$vint1[i]
  y      <- prep$data$Y[i]
  beta_binomial2_lpmf(y, mu, phi, trials)
}

posterior_predict_beta_binomial2 <- function(i, prep, ...) {
  mu     <- prep$dpars$mu[, i]
  phi    <- prep$dpars$phi
  trials <- prep$data$vint1[i]
  beta_binomial2_rng(mu, phi, trials)
}

# required to use `fitted()`
posterior_epred_beta_binomial2 <- function(prep) {
  mu     <- prep$dpars$mu
  trials <- prep$data$vint1
  trials <- matrix(trials, nrow = nrow(mu), ncol = ncol(mu), byrow = TRUE)
  mu * trials
}
```



```{r}
samples$predict <- predicted_draws(b12.1, newdata = dataAdmit)
# samples$predict

samples$predict_stats <- samples$predict %>%  ggdist::mean_qi(.width = 0.89) %>%
  mutate(case = seq_len(n()),
         p = .prediction / applications,
         p_lower = .lower / applications,
         p_upper = .upper / applications)

samples$epred <- epred_draws(b12.1, newdata = dataAdmit)

samples$epred_stats <- samples$epred %>%  ggdist::mean_qi(.width = 0.89) %>%
  mutate(case = seq_len(n()),
         p = .epred / applications,
         p_lower = .lower / applications,
         p_upper = .upper / applications)


p <- list()
p$plot <- ggplot(samples$predict_stats, aes(x = case, y = p)) +
  geom_linerange(aes(ymin = p_lower, ymax = p_upper),
                 size = 2.5, alpha = 1/4, color = "chartreuse4") +
  geom_pointrange(samples$epred_stats,
                  mapping = aes(x = case, y = p, ymin = p_lower, ymax = p_upper),
                  size = 1/2, shape = 1) +
  geom_point(dataAdmit %>% mutate(case = seq_len(n())), 
             mapping = aes(x = case, y = admit / applications),  
             size = 2, color = "blue") +
  scale_x_continuous(breaks = 1:12) +
  scale_y_continuous(breaks = 0:5 / 5, limits = c(0, 1)) +
  ggthemes::theme_hc() +
  theme(axis.ticks.x = element_blank(),
        legend.position = "none") +
  labs(Title = "Admission data",
       subtitle = "Posterior validation check",
       y = "admittance probability")
p$plot
```



### Negative-binomial or gamma-Poisson

**You absolutely need to look at the Poisson-lognormal mixture in
Kurtz's blog [Kurtz lognormal](https://solomonkurz.netlify.app/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/). 
See the added section below.**

#### Gamma-Poisson distribution shape

In terms of the shape $\alpha$ and rate $\beta$ the gamma distribution is

$$
\mathcal{Gamma}(y \mid\alpha, \beta) = \frac{\beta^\alpha y^{\alpha-1} e^{-\beta y}}{\Gamma(\alpha)}
$$

but the rate $\beta$ and scale $\theta$ are the reciprocal of each other.
Therefore the gamma distribution can be expressed in terms of shape $\alpha$
and scale $\theta$ as


$$
\mathcal{Gamma}(y \mid\alpha, \theta) = \frac{y^{\alpha-1} e^{-\frac{y}{\theta}}}{\theta^\alpha\Gamma(\alpha)}
$$

and, also, the gamma distribution can be expressed in terms of mean $\mu$ and
shape $\alpha$


$$
\mathcal{Gamma}(y \mid \mu, \alpha) = 
\frac{(\frac{\alpha}{\mu})^\alpha}{\Gamma(\alpha)}
y^{\alpha-1} \exp{(-\frac{\alpha y}{\mu})}
$$
To convert from the $\mu = mean$ and $\theta = dispersion= \frac{mean^2}{variance}$
to the shape and rate parameters we use the function `simstudy::gammaGetShapeRate()`.
To help us find the mean and dispersion to use with `simstudy::gammaGetShapeRate()`,
the custom function `gammaGetMeanDispersion` is also defined.  It is the inverse of 
`simstudy::gammaGetShapeRate()`.


```{r}
# custom function which is the inverse function of gammaGetShapeRate()
gammaGetMeanDispersion <- function(shape, rate) {
  stopifnot(shape > 0, rate > 0)
  dispersion <- 1 / shape
  mean <- shape / rate
  list("mean" = mean, "dispersion" = dispersion)
}

# test it
prm <- list()
prm <- within(prm, {
  values <- list(mean = 1, dispersion = 10)
  # get the shape and rate from the mean and dispersion
  sr <- gammaGetShapeRate(mean = values$mean, dispersion = values$dispersion)
  # using the inverse should take you back to the mean and dispersion
  md <- gammaGetMeanDispersion(shape = sr$shape, rate = sr$shape)
})
# using the inverse should take you back to the mean and dispersion
stopifnot(identical(prm$md, prm$values))
```


In the `dgamma` the shape parameter influence the and rate represents the equivalent
Poisson $lambda$

```{r}
p <- list()
p$df <- crossing(shape = c(0.1, 0.25, 0.5, 0.75, 1, 2), 
                 rate = c(1/10, 1/5, 1/2, 1)) %>%
  expand(nesting(shape, rate), 
         x = seq(from = 0, to = 5, length.out = 100)) %>%
  mutate(density = dgamma(x, shape, rate),
         shape_char    = paste("shape", shape, sep = "=="),
         rate_char = paste("rate", rate, sep = "=="))
```

```{r}
p$plot <- ggplot(data = p$df %>% filter(shape == 0.1, rate == 0.1),
                 aes(x = x, y = density)) +
  geom_area(fill = "orchid") +
  scale_y_continuous(NULL, labels = NULL) +
  labs(title = "Gamma prior with default values (shape = 0.1, rate = 0.1)",
       x = "parameter space") +
  ggthemes::theme_hc() +
  theme(axis.ticks.y = element_blank(),
        plot.background = element_rect(fill = "grey92"))
p$plot
```


and the plot with different values of sape and rate

```{r}
p$plot <- ggplot(data = p$df, aes(x = x, y = density, color = rate_char)) +
  geom_line(size = 1) +
  scale_y_continuous(NULL, labels = NULL) +
  scale_color_paletteer_d("ggthemes::excel_Atlas") +
  ggthemes::theme_hc() +
  theme(title = element_text(color = "midnightblue"),
        axis.text.x = element_text(size = rel(0.8)),
        axis.ticks.y = element_blank(),
        plot.background = element_rect(fill = "grey90"),
        legend.title = element_blank(),
        legend.background = element_rect(fill = "grey90")) +
  labs(title = "Gamma can take many shapes",
       x = "parameter space") +
  facet_wrap(. ~ shape_char, scales = "free_y", labeller = label_parsed)
p$plot
```


#### Data


```{r}
data(Kline)
dataKline <- Kline %>%
  mutate(population_ls = scale(log(population)),
         contact_id = ifelse(contact == "high", 2L, 1L),
         cid = contact)
```


#### Null model {-}

> This section is important as it serves to evaluate the prior to use
for the full model.  See how @kurtz2020b does it.  My work below does not
show everything (yet).

Start with the null model, or as Kurtz calls it, the intercept-only model.

$$
\begin{align*}
total\_tools_i &\sim \mathcal{GammaPoisson}(\mu, \alpha) \\
log(\mu) &= \beta_0 \\
\beta_0 &\sim \mathcal{Normal}(3, 0.5) \\
\alpha &\sim \mathcal{Gamma}(0.01,0.01)
\end{align*}
$$

and the fit

```{r}
a_file <- here::here("fits", "b12_02a.rds")
b12.2a <- readRDS(file = a_file)
# b12.2a <- brm(data = dataKline,
#       family = negbinomial,
#       total_tools ~ 1,
#       prior = c(prior(normal(3, 0.5), class = Intercept),  # beta_0
#                 prior(gamma(0.01, 0.01), class = shape)),  # alpha
#       cores = detectCores(),
#       seed = 12)
saveRDS(object = b12.2a, file = a_file)
```


with the summary

```{r}
summary(b12.2a)
```

Because the model has only the intercept and no predictor, there is only
one value for the Intercept which is the mean of the 10 possion rates 
$\lambda_i, i =1,...10$.

The $alpha$ is simply the $shape$ parameter of gamma ... and does not really
describe anything.  It is really used to define the shape of the distribution.

And the prediciton plots show that the distributions all use the same rate and 
shape.

```{r}
samples <- list()
samples$data <- predicted_draws(b12.2a, newdata = dataKline)
ggplot(samples$data, aes(.prediction, color = culture)) +
  geom_density(size = 1) +
  scale_y_continuous(NULL, labels = NULL) +
  scale_color_paletteer_d("ggthemes::calc") +
  ggthemes::theme_hc() +
  theme(title = element_text(color = "midnightblue"),
        axis.text.x = element_text(size = rel(0.8)),
        axis.ticks.y = element_blank(),
        plot.background = element_rect(fill = "grey90"),
        legend.position = "none",
        legend.background = element_rect(fill = "grey90"),
        strip.background = element_rect(fill = "bisque"),
        strip.text = element_text(color = "midnightblue")) +
  labs(title = "Predictive distributions",
       x = "total tools") +
  facet_wrap(. ~ culture)
```


and we can also visualize the distributions of our $rate$ and $shape$ parameters

```{r}
samples <- list()
samples$data <- as_draws_df(b12.2a) %>%
  select(b_Intercept, shape) %>%
  mutate(mu = exp(b_Intercept),
         theta = mu / shape) %>%
  select(mu, shape, theta) %>%
  pivot_longer(cols = everything())
# str(samples$data)
ggplot(samples$data, aes(value, fill = name, color = name)) +
  geom_density(geom = "area") +
  scale_y_continuous(NULL, labels = NULL) +
  scale_fill_paletteer_d("fishualize::Scarus_quoyi") +
  scale_color_paletteer_d("fishualize::Scarus_quoyi") +
  ggthemes::theme_hc() +
  theme(title = element_text(color = "midnightblue"),
        axis.text.x = element_text(size = rel(0.8)),
        axis.ticks.y = element_blank(),
        plot.background = element_rect(fill = "grey90"),
        legend.position = "none",
        legend.background = element_rect(fill = "grey90"),
        strip.background = element_rect(fill = "wheat"),
        strip.text = element_text(color = "midnightblue")) +
  labs(title = "Posterior distributions of rate and shape",
       x = NULL) +
  facet_wrap(. ~ name, scales = "free")
```



#### Full model {-}



$$
\begin{align*}
total\_tools_i &\sim \mathcal{GammaPoisson}(\mu_i, \alpha) \\
log(\mu) &= \frac{\exp{(\beta_{0,cid[i]})} \cdot population_i^{\beta_{1,cid[i]}}}{\gamma} \\
\beta_{0,j} &\sim \mathcal{Normal}(1, 1) \\
\beta_{1,j} &\sim \mathcal{Exponential}(1) \\
\gamma &\sim \mathcal{Exponential}(1) \\
\alpha &\sim \mathcal{Exponential}(1)
\end{align*}
$$



```{r}
a_file <- here::here("fits", "b12_02b.rds")
b12.2b <- readRDS(file = a_file)
# b12.2b <- brm(data = dataKline,
#       family = negbinomial(link = "identity"),
#       bf(total_tools ~ exp(b0) * population^b1 / g,
#          b0 + b1 ~ 0 + cid,
#          g ~ 1,
#          nl = TRUE),
#       prior = c(prior(normal(1, 1), nlpar = b0),
#                 prior(exponential(1), nlpar = b1, lb = 0),
#                 prior(exponential(1), nlpar = g, lb = 0),
#                 prior(exponential(1), class = shape)),
#       cores = detectCores(),
#       seed = 12,
#       control = list(adapt_delta = .95))
# saveRDS(object = b12.2b, file = a_file)
```

and the summary

```{r}
summary(b12.2b)
```

and the fitted values are

```{r}
samples <- list()
samples$data <- epred_draws(b12.2b, newdata = dataKline) %>%
  select(-c(".row", ".chain", ".iteration", ".draw"))
  # pivot_longer(cols = everything(), names)
str(samples$data)
  
  
# ggplot(samples$data, aes(value, fill = name, color = name)) +
#   geom_density(geom = "area") +
#   scale_y_continuous(NULL, labels = NULL) +
#   scale_fill_paletteer_d("fishualize::Scarus_quoyi") +
#   scale_color_paletteer_d("fishualize::Scarus_quoyi") +
#   ggthemes::theme_hc() +
#   theme(title = element_text(color = "midnightblue"),
#         axis.text.x = element_text(size = rel(0.8)),
#         axis.ticks.y = element_blank(),
#         plot.background = element_rect(fill = "grey90"),
#         legend.position = "none",
#         legend.background = element_rect(fill = "grey90"),
#         strip.background = element_rect(fill = "wheat"),
#         strip.text = element_text(color = "midnightblue")) +
#   labs(title = "Posterior distributions of rate and shape",
#        x = NULL) +
#   facet_wrap(. ~ name, scales = "free")
```



### Poisson-lognormal {-}

This is an extra section.  The result is so useful it is worth adding here. See
[Kurtz lognormal](https://solomonkurz.netlify.app/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/).

it was also saved in a local file called **Poisson-lognormal_mixture_Solomon Kurz.html**
in **C:\Users\Public\MyStudies\Rethinking_docs**

## Zero-inflated outcomes


### Zero-inflated Poisson

With zero-inflated Poisson both parameters $p$ and $\lambda$ can have their own
equation such as


$$
\begin{align*}
prod_i &\sim \mathcal{ZIPoisson}(p_i, \lambda_i) \\
logit(p_i) &= \alpha_p + \beta_p x_i \\
log(\lambda_i) &= \alpha_\lambda + \beta_\lambda x_i \\
\end{align*}
$$



```{r}
drink_prob <- 0.2  # drink 20% of the days
work_rate <- 1  # production = 1 manuscript per day

# simulate 1 year of drinking
set.seed(365)  # same seed as McElreath
drink <- rbinom(n = 365, size = 1, prob = drink_prob)
# simulate 1 year of work
# set.seed(365)
work <- rpois(n = length(drink), lambda = work_rate)
# simulate production
prod <- (1 - drink) * work

# create the dataframe
d <- data.frame(
  drink = factor(drink, levels = 1:0),
  prod = prod
  )
```


plot the data

```{r}
ggplot(d, aes(x = prod)) +
  geom_histogram(aes(fill = drink), binwidth = 1) +
  scale_fill_manual(values = paletteer::paletteer_d("ggthemr::fresh", direction = -1)) +
  ggthemes::theme_tufte()
```
#### Model and fit

$$
\begin{align*}
prod_i &\sim \mathcal{ZIPoisson}(p, \lambda) \\
p &= \alpha_p \\
log(\lambda) &= \alpha_\lambda \\
\alpha_p &\sim \mathcal{Beta}(2, 6) \\
\alpha_\lambda &\sim \mathcal{N}(1, 0.5)
\end{align*}
$$

In `brms`, $p_i$ is denoted `zi`. To use a non-default prior for `zi`, make
sure to indicate `class = zi`. **Important to read [Kurtz](kurtz2020b)**


```{r}
a_file <- here::here("fits", "b12_03.rds")
b12.3 <- readRDS(file = a_file)
# b12.3 <- brm(data = d, 
#       family = zero_inflated_poisson,
#       prod ~ 1,
#       prior = c(prior(normal(1, 0.5), class = Intercept),
#                 prior(beta(2, 6), class = zi)),  # the brms default is beta(1, 1)
#       cores = detectCores(),
#       seed = 12) 
# b12.3 <- add_criterion(b12.3, criterion = c("waic", "loo"))
# saveRDS(object = b12.3, file = a_file)
```


and the summary

```{r}
summary(b12.3)
```

with $\alpha_\lambda$ on natural scale being

```{r}
fixef(b12.3)
exp(fixef(b12.3))
```


## Ordered categorical outcomes


### Example: Moral intuition

```{r}
data(Trolley)
d <- Trolley
rm(Trolley)
skimr::skim(d)
```

and we can describe the data using the `summarytools` which does a great
job at creating that sort or report.



### Describing and ordered distribution with intercepts

The histogram of response

```{r}
p1 <- ggplot(d, aes(x = response)) +
  geom_histogram(aes(fill = ..count..), binwidth = 1) +
  scale_x_continuous(breaks = scales::breaks_width(width = 1)) +
  scale_fill_paletteer_c(palette = "pals::ocean.deep") +
  ggthemes::theme_tufte() +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "tan")) +
  labs(title = "Histogram of Trolley responses")
# p1
```

The cumulative proportions plot

```{r}
d.p2 <- d %>%
  count(response) %>%
  arrange(response) %>%
  mutate(pct = n / sum(n),
         cum_pct = cumsum(pct))
p2 <- ggplot(d.p2, aes(x = response, y = cum_pct)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = scales::breaks_width(width = 1)) +
  scale_y_continuous(breaks = scales::breaks_width(width = 0.2)) +
  ggthemes::theme_fivethirtyeight() +
  labs(title = "Cumulative proportions", y = "cumulative probabilities")
# p2
```

And the plot of `logit`

```{r}
d.p3 <- d %>%
  count(response) %>%
  mutate(pct = n / sum(n),
         cum_pct = cumsum(pct),
         logit = log(cum_pct / (1 - cum_pct)),
         logit_ctr = scale(logit, center = TRUE, scale = FALSE))
# d.p3
p3 <- ggplot(d.p3, aes(x = response, y = logit)) +
  geom_line() +
  geom_point() +
    scale_x_continuous(breaks = scales::breaks_width(width = 1)) +
  scale_y_continuous(breaks = scales::breaks_width(width = 1)) +
  ggthemes::theme_fivethirtyeight() +
  labs(title = "Log of Cumulative Odds",
       y = "log of cumulative odds (centered)")
# p3
```

and the 3 plots in figure 12.4 are

```{r}
(p1 + p2 + p3) +
  plot_annotation(title = "Figure 12.4")
```



The model is

$$
\begin{align*}
response_i &\sim \mathcal{Categorical}(\overrightarrow{p}) \\
logit(p_k) &= \alpha_k - \phi \\
\phi &= 0 \\
\alpha_k &\sim \mathcal{N}(0, 1.5)
\end{align*}
$$
and the fit with brms

```{r}
# define start values
inits <- list(
  `Intercept[1]` = -2,
  `Intercept[2]` = -1,
  `Intercept[3]` = 0,
  `Intercept[4]` = 1,
  `Intercept[5]` = 2,
  `Intercept[6]` = 2.5
)
inits_list <- list(inits, inits, inits, inits)
a_file <- here::here("fits", "b12_04.rds")
b12.4 <- readRDS(file = a_file)
# message("This takes at least 5 min.")
# b12.4 <- brm(
#   data = d,
#   family = cumulative,
#   response ~ 1,
#   prior = c(
#     prior(normal(0, 1.5), class = Intercept)),
#   cores = detectCores(),
#   # the start values
#   inits = inits_list,
#   seed = 12)
# b12.4 <- add_criterion(b12.4, c("loo", "waic"))
# saveRDS(object = b12.4, file = a_file)
```


which gives the summary

```{r}
print(b12.4)
```

and we convert the intercepts to the normal scale

```{r}
b12.4 %>%
  fixef() %>%
  brms::inv_logit_scaled()
# which we can also do with stats:plogis()
# see appendix A
# b12.4 %>%
#   fixef() %>%
#   stats::plogis()
```

**Important:** The SD i.e. `Est.Error` are not valid using the `inv_logit_scaled`,
that is using a direct inverse exp function.

They must be computed using a posterior sample.


```{r}
posterior_samples(b12.4) %>%
  select(starts_with("b_")) %>%
  mutate(across(.cols = everything(), .fns = brms::inv_logit_scaled)) %>%
  pivot_longer(cols = everything(), names_to = "key") %>%
  group_by(key) %>%
  summarise(mean = mean(value),
            sd = sd(value),
            ll = quantile(value, probs = 0.025),
            ul = quantile(value, probs = 0.975)) %>%
  mutate(across(.cols = where(is.numeric), .fns = round, digits = 4))
```


### Adding predictor variables

> Thsi form automatically ensure the correct ordering of the outcom values,
while still morphing the likelihood of each individual valueas the predictor $x_i$
changes value. Why is the linear model $\phi$ substracted from each intercept? 
Because if we decrease the log-cumulative-odds of every outcome value $k$ below
the maximum, this necessarily shifts probability mass upwards towards higher 
outcome values.

$$
\begin{align*}
\log{\left[ \frac{Pr(y_i \le k)}{1-Pr(y_i \le k)} \right]} &= \alpha_k - \phi_i \\
\phi_i &= \beta x_i
\end{align*}
$$

For example lets take model b12.4


```{r}
fixef(b12.4)
```


#### Logistic / Logit functions

See the appendix A of this book for a detailed treatment of all these functions.
They will be added the suffix *.new* to identify them.

The `logistic()` and `inv_logit()` functions are actually the same as 
`stats::plogis()`.

Also, the function `logit()` already exists as `stats::qlogis()`.

therefore `dordlogit()` as given

```{r}
dordlogit.new <- function(x, phi = 0L, log = FALSE) {
  x <- sort(x)  # the ordering is important
  p <- stats::plogis(q = c(x, Inf), location = phi)
  p <- c( p[1], p[2:length(p)] - p[1:(length(p)-1)] )
  if (log) p <- log(p)
  p
}
```

which gives about the same result as R code 11.9 in McElreath on p. 386
with R code 12.20, and Kurtz.

```{r}
probk <- dordlogit.new(fixef(b12.4)[, 1])
round(probk, 2)
```
which gives and expected value of

```{r}
sum(1:7 * probk)
```



#### Subsracting from the log-cumulative odds

If we substract from the *log-cumulative odds* then we shift the probability
mass to higher outcome values.

For example with model b12.4

```{r}
probk <- dordlogit.new(fixef(b12.4)[, 1])
round(probk, 2)
```
which gives an expected value

```{r}
sum(1:7 * probk)
```

but if we substract 0.5

```{r}
(dordlogit.new(fixef(b12.4)[, 1], phi = 0.5))
```
then we have a higher expected value

```{r}
sum(dordlogit.new(fixef(b12.4)[, 1], phi = 0.5) * 1:7)
```

#### Ordered categorical with several predictors

Our model with several predictors is

$$
\begin{align*}
response_i &\sim Categorical(\overrightarrow{p}) \\
logit(Pr(y_i \leq k)) &= \frac{Pr(y_i \leq k)}{1 - Pr(y_i \leq k)}  = \alpha_k - \phi_i \\
\phi_i &= \beta_{action} \cdot action_i + \beta_{intention} \cdot intention_i +  \beta_{contact} \cdot contact_i + \beta{a,i} \cdot(action_i \times intention_i) +
\beta{c,i} \cdot(contact_i \times intention_i) \\
\alpha_k &\sim \mathcal{N}(0, 1.5) \\
\beta_{\bullet} &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$


and the fit is


```{r}
a_file <- here::here("fits", "b12_05.rds")
b12.5 <- readRDS(file = a_file)
# message("This takes at least 10 min.")
# b12.5 <- brms::brm(data = d,
#                    family = cumulative,
#                    formula = response ~ 1 + action + intention + contact + 
#                      action:intention + contact:intention,
#                    prior = c(prior(normal(0, 1.5), class = Intercept),
#                              prior(normal(0, 0.5), class = b)),
#                    cores = detectCores(),
#                    # inits = list(inits, inits),
#                    seed = 12)
# b12.5 <- add_criterion(b12.5, c("loo", "waic"))
# saveRDS(object = b12.5, file = a_file)
```


the summary 

```{r}
summary(b12.5)
```


and plot the coefficients

```{r}
get_variables(b12.5)
```



```{r}

labs <- paste0("beta[", 1:5, "]")

b12.5_post <- gather_draws(model = b12.5, `b_action.*`, `b_contact.*`, `b_intention.*`,
                           regex = TRUE)

ggplot(b12.5_post, aes(x = .value, y = .variable)) +
  geom_vline(xintercept = 0, alpha = 1/5, linetype = 3) +
  stat_gradientinterval(.width = .5, size = 1, point_size = 3/2, shape = 21,
                      point_fill = "darkgreen",
                      fill ="green",
                      color = "darkgreen") +
  scale_x_continuous("marginal posterior", breaks = -5:0 / 4) +
  coord_cartesian(xlim = c(-1.4, 0)) +
  ggthemes::theme_hc() +
  labs(x = "marginal posterior", y = NULL,
       title = "Model b12.5 coefficients")
```


## Ordered categorical predictors

### Dirichlet distribution

The Dirichlet distribution, used in this section, can be illustrated as follows

```{r}
set.seed(1805)  # seed from McElreath
dp <- gtools::rdirichlet(10, alpha = rep(2, 7))  %>%
  data.frame() %>%
  setNames(1:7) %>%
  mutate(row = seq_len(nrow(.))) %>%
  pivot_longer(cols = -row, names_to = "index", values_to = "prob")

ggplot(dp, aes(x = index, y = prob, group = row)) +
  geom_line(aes(color = row == 3)) +
  geom_point(aes(color = row == 3)) +
  scale_color_manual(values = c("TRUE" = "darkgreen", "FALSE" = "lightgreen")) +
  ggthemes::theme_hc() +
  theme(legend.position = "none") +
  labs(title = "Dirichlet distribution",
       subtitle = "Figure 12.7",
       x = "index of variable in vector",
       y = "probability")
```

NOTE: The `brms` package also has a `rdirchlet()` function which is very useful
when investigating priors. See @kurtz2020b for details.


### Data


```{r}
data(Trolley)
d <- Trolley
rm(Trolley)
d <- d %>% 
  mutate(edu_new = 
           recode_factor(edu,
                  "Elementary School" = 1,
                  "Middle School" = 2,
                  "Some High School" = 3,
                  "High School Graduate" = 4,
                  "Some College" = 5, 
                  "Bachelor's Degree" = 6,
                  "Master's Degree" = 7,
                  "Graduate Degree" = 8,
                  .ordered = TRUE) %>% 
           as.integer())

d %>% 
  distinct(edu, edu_new) %>% 
  arrange(edu_new)
```


### Model and fit

The model is

$$
\begin{align*}
response_i &\sim \mathcal{Categorical}(\overrightarrow{\textbf{p}}) \\
logit(p_k) &= \alpha_k - \phi_i \\
\phi_i &= \beta_E \sum_{j=0}^{E_i-1} \delta_j + \beta_A \cdot action_i + \beta_I \cdot intention_i + \beta_C \cdot contact_i \\
\alpha_k &\sim \mathcal{N}(0,1.5) \\
\beta_A, \beta_I, \beta_C &\sim \mathcal{N}(0,1) \\
\beta_E &\sim \mathcal{N}(0, 0.143) \\
\overrightarrow{\mathbf{\delta}} &\sim \mathcal{Dirichlet}([2,2,2,2,2,2,2])
\end{align*}
$$



```{r}
a_file <- here::here("fits", "b12_06.rds")
b12.6 <- readRDS(file = a_file)
# message("This takes at least 90 minutes. Yes, 90 min.")
# b12.6 <- brm(data = d,
#       family = cumulative,
#       response ~ 1 + action + contact + intention + mo(edu_new),  # note the `mo()` syntax
#       prior = c(prior(normal(0, 1.5), class = Intercept),
#                 prior(normal(0, 1), class = b),
#                 # note the new kinds of prior statements
#                 # for monotonic variable edu_new
#                 prior(normal(0, 0.143), class = b, coef = moedu_new),
#                 prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu_new1)),
#       cores = detectCores(),
#       seed = 12)
# b12.6 <- add_criterion(b12.6, c("loo", "waic"))
# saveRDS(object = b12.6, file = a_file)
```

```{r}
summary(b12.6)
```
```{r}
delta_labels <- c("Elem", "MidSch", "SHS", "HSG", "SCol", "Bach", "Mast", "Grad")

dp <- posterior_samples(b12.6) %>% 
  select(contains("simo_moedu_new1")) %>% 
  setNames(paste0(delta_labels[2:8], "~(delta[", 1:7, "])"))

GGally::ggpairs(dp, labeller = label_parsed) +
  theme(strip.text = element_text(size = 8))
```



## Summary