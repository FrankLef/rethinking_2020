```{r include=FALSE}
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(tidybayes, quietly = TRUE)
library(rethinking)
library(brms)
library(loo)
library(dagitty, quietly = TRUE)
library(ggdag, quietly = TRUE)
library(ggdist, quietly = TRUE)
library(ggmcmc, quietly = TRUE)
library(bayesplot, quietly = TRUE)
library(patchwork, quietly = TRUE)
library(paletteer, quietly = TRUE)
```


# Monsters and Mixtures {#Mixed}


## Over-dispersed outcomes

### Beta-binomial

#### Beta-binomial distribution

The beta distribution is

$$
\mathcal{Beta}(x|\alpha, \beta) =
\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha -1} (1-x)^{\beta -1} =
\frac{1}{B(\alpha, \beta)} x^{\alpha -1} (1-x)^{\beta -1}, 0 \leq x \leq 1
$$
which is not the format used by McElrath.  He uses the following
shape parameters wich are much easier to understand as $\mu$ is the **average**
of the distribution and $\kappa$ is the **spread**.

$$
\mu = \bar{p} = \frac{\alpha}{\alpha + \beta} \\
\kappa = \theta = \alpha + \beta
$$

and the standard deviation of the beta binomial distribution is

$$
\sigma = \sqrt{\frac{\mu(1-\mu)}{\kappa+1}}
$$


The function to convert from the mean and kappa to the mathematical $\alpha$
and $\beta$, as provided by Krushke, is

```{r}
betaABfromMeanKappa <- function(mean, kappa) {
  stopifnot(mean > 0 & mean < 1)
  stopifnot(kappa > 0)
  a <- mean * kappa
  b <- (1 - mean) * kappa
  list(a = a, b = b)
}
```


The beta-binomial distribution is not defined in `brms`.  We need to define the 
family in ``brms` as well as a `stan_funs()` and `stanvar()`.

```{r}
# IMPORTANT: we could have used lb = c(NA, 0) as Brukner does
# see note just below on lb = c(NA, 2)
beta_binomial2 <- custom_family(
  "beta_binomial2", dpars = c("mu", "phi"),
  links = c("logit", "log"), lb = c(NA, 2),
  type = "int", vars = "vint1[n]"
)

stan_funs <- "
  real beta_binomial2_lpmf(int y, real mu, real phi, int T) {
    return beta_binomial_lpmf(y | T, mu * phi, (1 - mu) * phi);
  }
  int beta_binomial2_rng(real mu, real phi, int T) {
    return beta_binomial_rng(T, mu * phi, (1 - mu) * phi);
  }
"

stanvars <- stanvar(scode = stan_funs, block = "functions")
```


> Did you notice `lb = c(NA, 2`^ In Burkner's vignette the lower bound of $\phi$ 
is 0.  Since McElreath wanted the lower bound to 2, we will use lb = 2.

See also McEleath explanation of 2 in section 12.1.1 just before R code 12.1
on p. 371.


Variations of rhe beta-binomial distribution using different parameter values
can be illustrated as follows


```{r}
bpdf <- crossing(pbar = c(0.25, 0.5, 0.75), theta = c(5, 15, 30)) %>% 
  expand(nesting(pbar, theta), 
         x = seq(from = 0, to = 1, length.out = 100)) %>%
  mutate(density = rethinking::dbeta2(x, pbar, theta),
         mu = paste("mu", pbar, sep = "=="),
         kappa = paste("kappa", theta, sep = "=="))
bpdf
```

and the plot

```{r}
p <- ggplot(data = bpdf) +
  geom_ribbon(aes(x = x, ymin = 0, ymax = density),
              fill = "darkorchid1") + 
  scale_y_continuous(NULL, labels = NULL) +
  ggthemes::theme_hc() +
  theme(axis.ticks.y = element_blank(),
        plot.background = element_rect(fill = "grey92")) +
  labs(title = "Beta can take many shapes",
       x = "parameter space") +
  facet_grid(kappa~mu, labeller = label_parsed)
p
```



#### Beta-binomial model



The data used is

```{r}
data(UCBadmit)
d <- UCBadmit %>%
  mutate(gid = ifelse(applicant.gender == "male", "1", "2"))
rm(UCBadmit)
```



There is an error in the model defined by McElrath, to concur with his code
at 11.26, the model is

$$
\begin{align*}
admit_i &\sim \mathcal{BetaBinomial}(N_i, \bar{p}_i, \phi) \\
logit(\bar{p}_i) &= \alpha_{gid[i]} \\
\alpha &\sim \mathcal{N}(0, 1.5) \\
\phi &\sim \mathcal{Exponential}(1)
\end{align*}
$$


which we fit as follows

```{r}
a_file <- here::here("fits", "b12_01.rds")
b12.1 <- readRDS(file = a_file)
# b12.1 <- brm(
#   data = d,
#   family = beta_binomial2,
#   admit | vint(applications) ~ 0 + gid,
#   prior = c(prior(normal(0, 1.5), class = b),
#             prior(exponential(1), class = phi)),
#   cores = detectCores(),
#   stanvars = stanvars,
#   seed = 12)
# NOT USED b12.1 <- add_criterion(b12.1, criterion = c("waic", "loo"))
saveRDS(object = b12.1, file = a_file)
```

with the summary

```{r}
summary(b12.1)
```


and the posterior data which *represents the distribution rather than the data*

```{r}
b12.1_post <- posterior_samples(b12.1)
glimpse(b12.1_post)
```


and the interval of the difference between the coefficients is

```{r}
b12.1_post %>%
  transmute(b_diff = b_gid1 - b_gid2) %>%
  mean_qi(.width = 0.89) %>%
  mutate(across(.cols = where(is.double), .fns = round, digits = 2))
```

and so, just like McElreath, hthe difference between the admission rates
is close to zero.


#### Beta-binomial plots

See @kurtz2020b for the details

```{r}
set.seed(12)

b12.1_lines <-
  b12.1_post %>% 
  mutate(iter  = 1:n(),
         p_bar = inv_logit_scaled(b_gid2)) %>% 
  slice_sample(n = 100) %>% 
  expand(nesting(iter, p_bar, phi),
         x = seq(from = 0, to = 1, by = .005)) %>% 
  mutate(density = purrr::pmap_dbl(list(x, p_bar, phi), rethinking::dbeta2))

glimpse(b12.1_lines)
```

```{r}
ggplot(b12.1_lines, aes(x = x, y = density)) + 
  stat_function(fun = rethinking::dbeta2,
                args = list(prob = mean(inv_logit_scaled(b12.1_post[, "b_gid2"])),
                            theta = mean(b12.1_post[, "phi"])),
                size = 1.5, color = "lightblue") +
  geom_line(aes(group = iter),
            alpha = .2, color = "darkgreen") +
  scale_y_continuous(breaks = scales::breaks_width(width = 0.5), limits = c(0, 3)) +
  ggthemes::theme_few() +
  theme(panel.background = element_rect(fill = "wheat")) +
  labs(subtitle = "distribution of female admission rates",
       x = "probability admit", y = "density")
```



### Negative-binomial or gamma-Poisson

**You absolutely need to look at the Poisson-lognormal mixture in
Kurtz's blog [Kurtz blog](https://solomonkurz.netlify.app/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/). 
See the added section below.**

#### Gamma-Poisson distribution shape

In terms of the shape $\alpha$ and rate $\beta$ the gamma distribution is

$$
\mathcal{Gamma}(y \mid\alpha, \beta) = \frac{\beta^\alpha y^{\alpha-1} e^{-\beta y}}{\Gamma(\alpha)}
$$

but the rate $\beta$ and scale $\theta$ are the reciprocal of each other.
Therefore the gamma distribution can be expressed in terms of shape $\alpha$
and scale $\theta$ as


$$
\mathcal{Gamma}(y \mid\alpha, \theta) = \frac{y^{\alpha-1} e^{-\frac{y}{\theta}}}{\theta^\alpha\Gamma(\alpha)}
$$

and, also, the gamma distribution can be expressed in terms of mean $\mu$ and
shape $\alpha$



$$
\mathcal{Gamma}(y \mid \mu, \alpha) = 
\frac{(\frac{\alpha}{\mu})^\alpha}{\Gamma(\alpha)}
y^{\alpha-1} \exp{(-\frac{\alpha y}{\mu})}
$$


First we illustrate the gamma-poisson with different combinations of shape $\alpha$ and
scale $\theta$

Importnat note:  The `dgamma2()` was in the first edition of rethinking
but not the second edition.  It is shown here only for reference.

```{r}
gpdf <- crossing(mu    = c(1, 5, 9), theta = c(1, 5, 9)) %>% 
  expand(nesting(mu, theta), 
         x = seq(from = 0, to = 27, length.out = 100)) %>%
  mutate(density    = rethinking::dgamma2(x, mu, theta),
         mu_char    = paste("mu", mu, sep = "=="),
         theta_char = paste("theta", theta, sep = "=="))
# gpdf
```

and the plot

```{r}
p <- ggplot(data = gpdf) +
  geom_ribbon(aes(x = x, ymin = 0, ymax = density),
              fill = "maroon1") +
  geom_vline(aes(xintercept = mu),
             color = "maroon", linetype = 3) +
  scale_y_continuous(NULL, labels = NULL) +
  labs(title = "Gamma can take many shapes",
       subtitle = "(dotted vertical lines mark off the means)",
       x = "parameter space") +
  coord_cartesian(xlim = c(0, 25)) +
  ggthemes::theme_hc() +
  theme(axis.ticks.y = element_blank(),
        plot.background = element_rect(fill = "grey92")) +
  facet_grid(theta_char~mu_char, labeller = label_parsed)
p
```




#### Data


```{r}
data(Kline)
d <- Kline %>%
  mutate(population_ls = scale(log(population)),
         contact_id = ifelse(contact == "high", 2L, 1L),
         cid = contact)
# d
```

#### Null model


Lets do it the right way.  Start with the null model, or as Kurtz call it,
the intercept-only model

$$
\begin{align*}
total\_tools_i &\sim \mathcal{GammaPoisson}(\mu, \alpha) \\
log(\mu) &= \beta_0 \\
\beta_0 &\sim \mathcal{Normal}(3, 0.5) \\
\alpha &\sim \mathcal{Gamma}(0.01,0.01)
\end{align*}
$$

and the fit

```{r}
a_file <- here::here("fits", "b12_02null.rds")
b12.2null <- readRDS(file = a_file)
# b12.2null <- brm(data = d, 
#       family = negbinomial,
#       total_tools ~ 1,
#       prior = c(prior(normal(3, 0.5), class = Intercept),  # beta_0
#                 prior(gamma(0.01, 0.01), class = shape)),  # alpha
#       cores = detectCores(),
#       seed = 12)
# b12.2null <- add_criterion(b12.2null, criterion = c("waic", "loo"))
# saveRDS(object = b12.2null, file = a_file)
```


with the summary

```{r}
summary(b12.2null)
```

#### Full model



$$
\begin{align*}
total\_tools_i &\sim \mathcal{GammaPoisson}(\mu_i, \alpha) \\
log(\mu) &= \frac{\exp{(\beta_{0,cid[i]})} \cdot population_i^{\beta_{1,cid[i]}}}{\gamma} \\
\beta_{0,j} &\sim \mathcal{Normal}(1, 1) \\
\beta_{1,j} &\sim \mathcal{Exponential}(1) \\
\gamma &\sim \mathcal{Exponential}(1) \\
\alpha &\sim \mathcal{Exponential}(1)
\end{align*}
$$



```{r}
a_file <- here::here("fits", "b12_02.rds")
b12.2 <- readRDS(file = a_file)
# b12.2 <- brm(data = d, 
#       family = negbinomial(link = "identity"),
#       bf(total_tools ~ exp(b0) * population^b1 / g,
#          b0 + b1 ~ 0 + cid,
#          g ~ 1,
#          nl = TRUE),
#       prior = c(prior(normal(1, 1), nlpar = b0),
#                 prior(exponential(1), nlpar = b1, lb = 0),
#                 prior(exponential(1), nlpar = g, lb = 0),
#                 prior(exponential(1), class = shape)),
#       cores = detectCores(),
#       seed = 12,
#       control = list(adapt_delta = .95))
# b12.2 <- add_criterion(b12.2, criterion = c("waic", "loo"))
# saveRDS(object = b12.2, file = a_file)
```

and the summary

```{r}
summary(b12.2)
```


### Poisson-lognormal

This is an extra section.  The result is so useful it is worth adding here. See
[Kurtz blog](https://solomonkurz.netlify.app/post/2021-07-12-got-overdispersion-try-observation-level-random-effects-with-the-poisson-lognormal-mixture/).

it was also saved in a local file called **Poisson-lognormal_mixture_Solomon Kurz.html**
in **C:\Users\Public\MyStudies\Rethinking_docs**

## Zero-inflated outcomes


### Zero-inflated Poisson

With zero-inflated Poisson both parameters $p$ and $\lambda$ can have their own
equation such as


$$
\begin{align*}
prod_i &\sim \mathcal{ZIPoisson}(p_i, \lambda_i) \\
logit(p_i) &= \alpha_p + \beta_p x_i \\
log(\lambda_i) &= \alpha_\lambda + \beta_\lambda x_i \\
\end{align*}
$$



```{r}
drink_prob <- 0.2  # drink 20% of the days
work_rate <- 1  # production = 1 manuscript per day

# simulate 1 year of drinking
set.seed(365)  # same seed as McElreath
drink <- rbinom(n = 365, size = 1, prob = drink_prob)
# simulate 1 year of work
# set.seed(365)
work <- rpois(n = length(drink), lambda = work_rate)
# simulate production
prod <- (1 - drink) * work

# create the dataframe
d <- data.frame(
  drink = factor(drink, levels = 1:0),
  prod = prod
  )
```


plot the data

```{r}
ggplot(d, aes(x = prod)) +
  geom_histogram(aes(fill = drink), binwidth = 1) +
  scale_fill_manual(values = paletteer::paletteer_d("ggthemr::fresh", direction = -1)) +
  ggthemes::theme_tufte()
```
#### Model and fit

$$
\begin{align*}
prod_i &\sim \mathcal{ZIPoisson}(p, \lambda) \\
p &= \alpha_p \\
log(\lambda) &= \alpha_\lambda \\
\alpha_p &\sim \mathcal{Beta}(2, 6) \\
\alpha_\lambda &\sim \mathcal{N}(1, 0.5)
\end{align*}
$$

In `brms`, $p_i$ is denoted `zi`. To use a non-default prior for `zi`, make
sure to indicate `class = zi`. **Important to read [Kurtz](kurtz2020b)**


```{r}
a_file <- here::here("fits", "b12_03.rds")
b12.3 <- readRDS(file = a_file)
# b12.3 <- brm(data = d, 
#       family = zero_inflated_poisson,
#       prod ~ 1,
#       prior = c(prior(normal(1, 0.5), class = Intercept),
#                 prior(beta(2, 6), class = zi)),  # the brms default is beta(1, 1)
#       cores = detectCores(),
#       seed = 12) 
# b12.3 <- add_criterion(b12.3, criterion = c("waic", "loo"))
# saveRDS(object = b12.3, file = a_file)
```


and the summary

```{r}
summary(b12.3)
```

with $\alpha_\lambda$ on natural scale being

```{r}
fixef(b12.3)
exp(fixef(b12.3))
```


## Ordered categorical outcomes


### Example: Moral intuition

```{r}
data(Trolley)
d <- Trolley
rm(Trolley)
skimr::skim(d)
```

and we can describe the data using the `summarytools` which does a great
job at creating that sort or report.



### Describing and ordered distribution with intercepts

The histogram of response

```{r}
p1 <- ggplot(d, aes(x = response)) +
  geom_histogram(aes(fill = ..count..), binwidth = 1) +
  scale_x_continuous(breaks = scales::breaks_width(width = 1)) +
  scale_fill_paletteer_c(palette = "pals::ocean.deep") +
  ggthemes::theme_tufte() +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "tan")) +
  labs(title = "Histogram of Trolley responses")
# p1
```

The cumulative proportions plot

```{r}
d.p2 <- d %>%
  count(response) %>%
  arrange(response) %>%
  mutate(pct = n / sum(n),
         cum_pct = cumsum(pct))
p2 <- ggplot(d.p2, aes(x = response, y = cum_pct)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = scales::breaks_width(width = 1)) +
  scale_y_continuous(breaks = scales::breaks_width(width = 0.2)) +
  ggthemes::theme_fivethirtyeight() +
  labs(title = "Cumulative proportions", y = "cumulative probabilities")
# p2
```

And the plot of `logit`

```{r}
d.p3 <- d %>%
  count(response) %>%
  mutate(pct = n / sum(n),
         cum_pct = cumsum(pct),
         logit = log(cum_pct / (1 - cum_pct)),
         logit_ctr = scale(logit, center = TRUE, scale = FALSE))
# d.p3
p3 <- ggplot(d.p3, aes(x = response, y = logit)) +
  geom_line() +
  geom_point() +
    scale_x_continuous(breaks = scales::breaks_width(width = 1)) +
  scale_y_continuous(breaks = scales::breaks_width(width = 1)) +
  ggthemes::theme_fivethirtyeight() +
  labs(title = "Log of Cumulative Odds",
       y = "log of cumulative odds (centered)")
# p3
```

and the 3 plots in figure 12.4 are

```{r}
(p1 + p2 + p3) +
  plot_annotation(title = "Figure 12.4")
```



The model is

$$
\begin{align*}
response_i &\sim \mathcal{Categorical}(\overrightarrow{p}) \\
logit(p_k) &= \alpha_k - \phi \\
\phi &= 0 \\
\alpha_k &\sim \mathcal{N}(0, 1.5)
\end{align*}
$$
and the fit with brms

```{r}
# define start values
inits <- list(
  `Intercept[1]` = -2,
  `Intercept[2]` = -1,
  `Intercept[3]` = 0,
  `Intercept[4]` = 1,
  `Intercept[5]` = 2,
  `Intercept[6]` = 2.5
)
inits_list <- list(inits, inits, inits, inits)
a_file <- here::here("fits", "b12_04.rds")
b12.4 <- readRDS(file = a_file)
# message("This takes at least 5 min.")
# b12.4 <- brm(
#   data = d,
#   family = cumulative,
#   response ~ 1,
#   prior = c(
#     prior(normal(0, 1.5), class = Intercept)),
#   cores = detectCores(),
#   # the start values
#   inits = inits_list,
#   seed = 12)
# b12.4 <- add_criterion(b12.4, c("loo", "waic"))
# saveRDS(object = b12.4, file = a_file)
```


which gives the summary

```{r}
print(b12.4)
```

and we convert the intercepts to the normal scale

```{r}
b12.4 %>%
  fixef() %>%
  brms::inv_logit_scaled()
# which we can also do with stats:plogis()
# see appendix A
# b12.4 %>%
#   fixef() %>%
#   stats::plogis()
```
**Important:** The SD i.e. `Est.Error` are not valid using the `inv_logit_scaled`,
that is using a direct inverse exp function.

They must be computed using a posterior sample.


```{r}
posterior_samples(b12.4) %>%
  select(starts_with("b_")) %>%
  mutate(across(.cols = everything(), .fns = brms::inv_logit_scaled)) %>%
  pivot_longer(cols = everything(), names_to = "key") %>%
  group_by(key) %>%
  summarise(mean = mean(value),
            sd = sd(value),
            ll = quantile(value, probs = 0.025),
            ul = quantile(value, probs = 0.975)) %>%
  mutate(across(.cols = where(is.numeric), .fns = round, digits = 4))
```


### Adding predictor variables

> Thsi form automatically ensure the correct ordering of the outcom values,
while still morphing the likelihood of each individual valueas the predictor $x_i$
changes value. Why is the linear model $\phi$ substracted from each intercept? 
Because if we decrease the log-cumulative-odds of every outcome value $k$ below
the maximum, this necessarily shifts probability mass upwards towards higher 
outcome values.

$$
\begin{align*}
\log{\left[ \frac{Pr(y_i \le k)}{1-Pr(y_i \le k)} \right]} &= \alpha_k - \phi_i \\
\phi_i &= \beta x_i
\end{align*}
$$

For example lets take model b12.4


```{r}
fixef(b12.4)
```


#### Logistic / Logit functions

See the appendix A of this book for a detailed treatment of all these functions.
They will be added the suffix *.new* to identify them.

The `logistic()` and `inv_logit()` functions are actually the same as 
`stats::plogis()`.

Also, the function `logit()` already exists as `stats::qlogis()`.

therefore `dordlogit()` as given

```{r}
dordlogit.new <- function(x, phi = 0L, log = FALSE) {
  x <- sort(x)  # the ordering is important
  p <- stats::plogis(q = c(x, Inf), location = phi)
  p <- c( p[1], p[2:length(p)] - p[1:(length(p)-1)] )
  if (log) p <- log(p)
  p
}
```

which gives about the same result as R code 11.9 in McElreath on p. 386
with R code 12.20, and Kurtz.

```{r}
probk <- dordlogit.new(fixef(b12.4)[, 1])
round(probk, 2)
```
which gives and expected value of

```{r}
sum(1:7 * probk)
```



#### Subsracting from the log-cumulative odds

If we substract from the *log-cumulative odds* then we shift the probability
mass to higher outcome values.

For example with model b12.4

```{r}
probk <- dordlogit.new(fixef(b12.4)[, 1])
round(probk, 2)
```
which gives an expected value

```{r}
sum(1:7 * probk)
```

but if we substract 0.5

```{r}
(dordlogit.new(fixef(b12.4)[, 1], phi = 0.5))
```
then we have a higher expected value

```{r}
sum(dordlogit.new(fixef(b12.4)[, 1], phi = 0.5) * 1:7)
```

#### Ordered categorical with several predictors

Our model with several predictors is

$$
\begin{align*}
response_i &\sim Categorical(\overrightarrow{p}) \\
logit(Pr(y_i \leq k)) &= \frac{Pr(y_i \leq k)}{1 - Pr(y_i \leq k)}  = \alpha_k - \phi_i \\
\phi_i &= \beta_{action} \cdot action_i + \beta_{intention} \cdot intention_i +  \beta_{contact} \cdot contact_i + \beta{a,i} \cdot(action_i \times intention_i) +
\beta{c,i} \cdot(contact_i \times intention_i) \\
\alpha_k &\sim \mathcal{N}(0, 1.5) \\
\beta_{\bullet} &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$


and the fit is


```{r}
a_file <- here::here("fits", "b12_05.rds")
b12.5 <- readRDS(file = a_file)
# message("This takes at least 10 min.")
# b12.5 <- brms::brm(data = d,
#                    family = cumulative,
#                    formula = response ~ 1 + action + intention + contact + 
#                      action:intention + contact:intention,
#                    prior = c(prior(normal(0, 1.5), class = Intercept),
#                              prior(normal(0, 0.5), class = b)),
#                    cores = detectCores(),
#                    # inits = list(inits, inits),
#                    seed = 12)
# b12.5 <- add_criterion(b12.5, c("loo", "waic"))
# saveRDS(object = b12.5, file = a_file)
```


the summary 

```{r}
summary(b12.5)
```


and plot the coefficients

```{r}
get_variables(b12.5)
```



```{r}

labs <- paste0("beta[", 1:5, "]")

b12.5_post <- gather_draws(model = b12.5, `b_action.*`, `b_contact.*`, `b_intention.*`,
                           regex = TRUE)

ggplot(b12.5_post, aes(x = .value, y = .variable)) +
  geom_vline(xintercept = 0, alpha = 1/5, linetype = 3) +
  stat_gradientinterval(.width = .5, size = 1, point_size = 3/2, shape = 21,
                      point_fill = "darkgreen",
                      fill ="green",
                      color = "darkgreen") +
  scale_x_continuous("marginal posterior", breaks = -5:0 / 4) +
  coord_cartesian(xlim = c(-1.4, 0)) +
  ggthemes::theme_hc() +
  labs(x = "marginal posterior", y = NULL,
       title = "Model b12.5 coefficients")
```


## Ordered categorical predictors

### Dirichlet distribution

The Dirichlet distribution, used in this section, can be illustrated as follows

```{r}
set.seed(1805)  # seed from McElreath
dp <- gtools::rdirichlet(10, alpha = rep(2, 7))  %>%
  data.frame() %>%
  setNames(1:7) %>%
  mutate(row = seq_len(nrow(.))) %>%
  pivot_longer(cols = -row, names_to = "index", values_to = "prob")

ggplot(dp, aes(x = index, y = prob, group = row)) +
  geom_line(aes(color = row == 3)) +
  geom_point(aes(color = row == 3)) +
  scale_color_manual(values = c("TRUE" = "darkgreen", "FALSE" = "lightgreen")) +
  ggthemes::theme_hc() +
  theme(legend.position = "none") +
  labs(title = "Dirichlet distribution",
       subtitle = "Figure 12.7",
       x = "index of variable in vector",
       y = "probability")
```

NOTE: The `brms` package also has a `rdirchlet()` function which is very useful
when investigating priors. See @kurtz2020b for details.


### Data


```{r}
data(Trolley)
d <- Trolley
rm(Trolley)
d <- d %>% 
  mutate(edu_new = 
           recode_factor(edu,
                  "Elementary School" = 1,
                  "Middle School" = 2,
                  "Some High School" = 3,
                  "High School Graduate" = 4,
                  "Some College" = 5, 
                  "Bachelor's Degree" = 6,
                  "Master's Degree" = 7,
                  "Graduate Degree" = 8,
                  .ordered = TRUE) %>% 
           as.integer())

d %>% 
  distinct(edu, edu_new) %>% 
  arrange(edu_new)
```


### Model and fit

The model is

$$
\begin{align*}
response_i &\sim \mathcal{Categorical}(\overrightarrow{p}) \\
logit(p_k) &= \alpha_k - \phi_i \\
\phi_i &= \beta_E \sum_{j=0}^{E_i-1} \delta_j + \beta_A \cdot action_i + \beta_I \cdot intention_i + \beta_C \cdot contact_i \\
\alpha_k &\sim \mathcal{N}(0,1.5) \\
\beta_A, \beta_I, \beta_C &\sim \mathcal{N}(0,1) \\
\beta_E &\sim \mathcal{N}(0, 0.143) \\
\overrightarrow{\delta} &\sim \mathcal{Dirichlet}([2,2,2,2,2,2,2])
\end{align*}
$$



```{r}
a_file <- here::here("fits", "b12_06.rds")
b12.6 <- readRDS(file = a_file)
# message("This takes at least 90 minutes. Yes, 90 min.")
# b12.6 <- brm(data = d,
#       family = cumulative,
#       response ~ 1 + action + contact + intention + mo(edu_new),  # note the `mo()` syntax
#       prior = c(prior(normal(0, 1.5), class = Intercept),
#                 prior(normal(0, 1), class = b),
#                 # note the new kinds of prior statements
#                 # for monotonic variable edu_new
#                 prior(normal(0, 0.143), class = b, coef = moedu_new),
#                 prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu_new1)),
#       cores = detectCores(),
#       seed = 12)
# b12.6 <- add_criterion(b12.6, c("loo", "waic"))
# saveRDS(object = b12.6, file = a_file)
```

```{r}
summary(b12.6)
```
```{r}
library(GGally)

delta_labels <- c("Elem", "MidSch", "SHS", "HSG", "SCol", "Bach", "Mast", "Grad")

dp <- posterior_samples(b12.6) %>% 
  select(contains("simo_moedu_new1")) %>% 
  setNames(paste0(delta_labels[2:8], "~(delta[", 1:7, "])"))

GGally::ggpairs(dp, labeller = label_parsed) +
  theme(strip.text = element_text(size = 8))
```



## Summary