```{r include=FALSE}
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(tidybayes, quietly = TRUE)
library(rethinking)
library(brms)
library(loo)
library(dagitty, quietly = TRUE)
library(ggdag, quietly = TRUE)
library(ggdist, quietly = TRUE)
library(ggmcmc, quietly = TRUE)
library(bayesplot, quietly = TRUE)
library(patchwork, quietly = TRUE)
library(paletteer, quietly = TRUE)
```


# Counting and Classification {#Counting}



## Binomial regression

### Logistic regression: Prosocial chimpanzees

Load the data


```{r}
data(chimpanzees)
d <- chimpanzees
rm(chimpanzees)
d %>%
  distinct(prosoc_left, condition) %>%
  mutate(treatment = 1 + prosoc_left + 2 * condition,
         description = c("Alone and two food items on the right",
                         "Alone and two food items on the left",
                         "Partner and two food items on the right",
                         "Partner and two food items on the left"))
```

Create a variable `treatment` that is a
code for `prosoc_left` and `condition` variables with the following meanings


```{r}
d <- d %>%
  mutate(actor = factor(actor),
         treatment = factor(1 + prosoc_left + 2 * condition, levels = 1:4, ordered = TRUE),
         labels = factor(treatment, levels = 1:4, labels = c("a/r", "a/l", "p/r", "p/l")),
         side = factor(prosoc_left + 1),
         cond = factor(condition + 1))
skimr::skim(d)
```

The model used will be

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha_{actor[i]} + \beta_{treatment[i]} \\
\alpha_j &\sim \mathcal{N}(0, sd) \\
\beta_k &\sim \mathcal{N}(0, sd) \\
&\text{sd to be determined}
\end{align*}
$$


#### Prior for $\alpha$


We begin with the one-intercept only model, the "null" model as we can maybe call
it.  It refers to a general mean for all $p_i$.

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha \\
\alpha &\sim \mathcal{N}(0, 10)
\end{align*}
$$


in `brm( forml = pulled_left | trials(1) ~ 1)` the `|` indicates we have extra
information about the criterion. In this case, that information is that each
`pulled_left` corresponds to a single trial, i.e. `trials(1)` which corresponds
to the $n = 1$ in $Binomial(1, p_i)$

We will use 2 sd values for $\alpha \sim \mathcal{N}(0, sd)$.

```{r}
prior_alpha_sd <- c(10, 1.5)
prior_alpha_sd
```


```{r}
a_file <- here::here("fits", "b11_01.rds")
b11.1 <- readRDS(file = a_file)
# b11.1 <- brm(data = d,
#               family = binomial,
#               formula = pulled_left | trials(1) ~ 1,
#               prior = c(
#                 prior(normal(0, 10), class = Intercept)
#                 ),
#               seed = 11, cores = detectCores(),
#               sample_prior = TRUE
#               )
# b11.1 <- brms::add_criterion(b11.1, criterion = c("waic", "loo"))
# saveRDS(b11.1, file = a_file)
summary(b11.1)
# convert the result using the inverse link funciton,
# in this case the logistic funciton or inv_logit()
brms::inv_logit_scaled(fixef(b11.1))
```

and we convert the result using `brms::inv_logit_scaled()`.

```{r}
brms::inv_logit_scaled(fixef(b11.1))
```

and we use another value for the prior to be able to calibrate it.


$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha \\
\alpha &\sim \mathcal{N}(0, 1.5)
\end{align*}
$$


```{r}
a_file <- here::here("fits", "b11_01b.rds")
b11.1b <- readRDS(file = a_file)
# b11.1b <- brm(data = d,
#               family = binomial,
#               formula = pulled_left | trials(1) ~ 1,
#               prior = c(
#                 prior(normal(0, 1.5), class = Intercept)
#                 ),
#               seed = 11, cores = detectCores(),
#               sample_prior = TRUE
#               )
# b11.1b <- brms::add_criterion(b11.1b, criterion = c("waic", "loo"))
# saveRDS(b11.1b, file = a_file)
summary(b11.1b)
brms::inv_logit_scaled(fixef(b11.1b))
```


and we visualize the outcome using the prior samples


```{r}
b11.1_prior <- prior_samples(b11.1)
b11.1b_prior <- prior_samples(b11.1b)
b11.1_prior_df <- bind_rows(b11.1_prior, b11.1b_prior) %>%
  mutate(p = inv_logit_scaled(Intercept),
         id = rep(c("sd = 10", "sd = 1.5"), each = n() / 2))

ggplot(b11.1_prior_df, aes(x = p, fill = id, color = id)) +
  geom_density(aes(y = ..count..), alpha = 3/4) +
  scale_fill_paletteer_d("lisa::PavelTchelitchew") +
  scale_color_paletteer_d("lisa::PavelTchelitchew") +
  ggthemes::theme_gdocs() +
  theme(legend.position = c(0.5, 0.8)) +
  labs(title = "Density of pulled_left with prior",
       subtitle = "Model 11.1 and 11.1b",
       x = "prob. of pulled_left")
```


#### Prior for $\beta_{treatment[i]}$

Now we find the sd value for the prior $\beta_k &\sim \mathcal{N}(0, sd)$.
Note that Solomon Kurtz in @kurtz2020b uses `inv_logit_scaled()` whereas
mcElreath in @elreath2020 uses `inv_logit()`.

$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha + \beta_{treatment[i]} \\
\alpha &\sim \mathcal{N}(0, 1.5) \\
\beta_k &\sim \mathcal{N}(0, sd) \\
&\text{sd to be determined}
\end{align*}
$$

We get the fit with $sd = 10$


```{r}
a_file <- here::here("fits", "b11_02.rds")
b11.2 <- readRDS(file = a_file)
# b11.2 <- brm(data = d,
#               family = binomial,
#               formula = bf(pulled_left | trials(1) ~ a + b,
#                            a ~ 1,
#                            b ~ 0 + treatment,
#                            nl = TRUE),
#               prior = c(
#                 prior(normal(0, 1.5), nlpar = a),
#                 prior(normal(0, 10), nlpar = b, coef = treatment1),
#                 prior(normal(0, 10), nlpar = b, coef = treatment2),
#                 prior(normal(0, 10), nlpar = b, coef = treatment3),
#                 prior(normal(0, 10), nlpar = b, coef = treatment4)
#                 ),
#               sample_prior = TRUE,
#              cores = detectCores(), seed = 11)
# b11.2 <- brms::add_criterion(b11.2, criterion = c("waic", "loo"))
# saveRDS(b11.2, file = a_file)
summary(b11.2)
brms::inv_logit_scaled(fixef(b11.2))
```



then the fit with $sd = 0.5$

```{r}
a_file <- here::here("fits", "b11_03.rds")
b11.3 <- readRDS(file = a_file)
# b11.3 <- brm(data = d,
#               family = binomial,
#               formula = bf(pulled_left | trials(1) ~ a + b,
#                            a ~ 1,
#                            b ~ 0 + treatment,
#                            nl = TRUE),
#               prior = c(
#                 prior(normal(0, 1.5), nlpar = a),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment1),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment2),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment3),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment4)
#                 ),
#               sample_prior = TRUE,
#              cores = detectCores(), seed = 11)
# b11.3 <- brms::add_criterion(b11.3, criterion = c("waic", "loo"))
# saveRDS(b11.3, file = a_file)
summary(b11.3)
brms::inv_logit_scaled(fixef(b11.3))
```

```{r}
b11.2_prior <- prior_samples(b11.2)
b11.3_prior <- prior_samples(b11.3)

b11.2_prior_df <- bind_rows(b11.2_prior, b11.3_prior) %>%
  mutate(p1 = inv_logit_scaled(b_a + b_b_treatment1),
         p2 = inv_logit_scaled(b_a + b_b_treatment2),
         diff = abs(p1 - p2),
         id = rep(c("sd = 10", "sd = 0.5"), each = n() / 2))

ggplot(b11.2_prior_df, aes(x = diff, fill = id, color = id)) +
  geom_density(aes(y = ..count..), alpha = 3/4) +
  scale_fill_paletteer_d("lisa::BarnettNewman", direction = 1) +
  scale_color_paletteer_d("lisa::BarnettNewman", direction = 1) +
  ggthemes::theme_gdocs() +
  theme(legend.position = c(0.5, 0.8)) +
  labs(title = "Density of difference: pulled_left with prior",
       subtitle = "Model 11.2 and 11.3",
       x = "prob. of pulled_left")
```

#### The full model

Now that we have investigated the prior, let's do the full model with them


$$
\begin{align*}
pulled\_left_i &\sim \mathcal{Binomial}(1, p_i) \\
logit(p_i) &= \alpha_{actor[i]} + \beta_{treatment[i]} \\
\alpha_j &\sim \mathcal{N}(0, 1.5) \\
\beta_k &\sim \mathcal{N}(0, 0.5)
\end{align*}
$$



```{r}
a_file <- here::here("fits", "b11_04.rds")
b11.4 <- readRDS(file = a_file)
# b11.4 <- brm(data = d,
#               family = binomial,
#               formula = bf(pulled_left | trials(1) ~ a + b,
#                            a ~ 0 + actor,
#                            b ~ 0 + treatment,
#                            nl = TRUE),
#               prior = c(
#                 prior(normal(0, 1.5), nlpar = a),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment1),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment2),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment3),
#                 prior(normal(0, 0.5), nlpar = b, coef = treatment4)
#                 ),
#              cores = detectCores(), seed = 11)
# b11.4 <- brms::add_criterion(b11.4, criterion = c("waic", "loo"))
# saveRDS(b11.4, file = a_file)
summary(b11.4)
brms::inv_logit_scaled(fixef(b11.4))
```

```{r}
get_variables(b11.4)
```

for the coefficients related to actors we have

```{r}
# NOTE: do not do actors and treatments together because of inv_logit_scaled
df <- posterior_samples(b11.4) %>%
  select(starts_with("b_a_")) %>%
  pivot_longer(cols = everything(), names_to = "actor") %>%
  mutate(prob = brms::inv_logit_scaled(value),
         id = factor(gsub(pattern = "^b_a_", replacement = "", x = actor))) %>%
  group_by(id) %>%
  ggdist::mean_qi(prob, .width = 0.95)
# glimpse(df)


ggplot(df, aes(x = prob, y = id)) +
  geom_pointintervalh(color = "darkgreen") +
  ggrepel::geom_text_repel(aes(label = round(prob, 2)), size = 3) +
  # theme_minimal() +
  ggthemes::theme_hc() +
  theme(title = element_text(color = "midnightblue")
        ) +
  labs(title = "probability (logistic / inv_logit) coefficient for actors",
       subtitle = "Model b11.4",
       x = "probability / proportion of time", y = NULL)
```

and for treatment

```{r}
get_variables(b11.4)
```

```{r}
df <- posterior_samples(b11.4) %>%
  select(starts_with("b_b_")) %>%
  pivot_longer(cols = everything(), names_to = "treatment") %>%
  mutate(prob = brms::inv_logit_scaled(value),
         id = gsub(pattern = "^b_b_", replacement = "", x = treatment),
         label = case_when(
           treatment == "b_b_treatment1" ~ "A/R",
           treatment == "b_b_treatment2" ~ "A/L",
           treatment == "b_b_treatment3" ~ "P/R",
           treatment == "b_b_treatment4" ~ "P/L",
           TRUE ~ as.character(treatment)
         )) %>%
  group_by(label) %>%
  ggdist::mean_qi(value, .width = 0.95)
# glimpse(df)

ggplot(df, aes(x = value, y = label)) +
  geom_pointintervalh(color = "goldenrod") +
  ggrepel::geom_text_repel(aes(label = round(value, 2)), size = 3) +
  ggthemes::theme_hc() +
  theme(title = element_text(color = "midnightblue")
        ) +
  labs(title = "logit coefficient for treatments",
       subtitle = "Model b11.4",
       x = "logit", y = NULL)
```


to compare the models

```{r}
w <- loo_compare(b11.2, b11.3, b11.4, criterion = "waic") %>%
  as.data.frame() %>%
  mutate(waic_diff = elpd_diff * -2,
         waic_diff_se = se_diff * 2) %>%
  round(digits = 1) %>%
  tibble::rownames_to_column(var = "model")
print(w, simplify = FALSE)
```

and the coefficient plot is

```{r}
ggplot(w, aes(x = reorder(model, waic), color = model)) +
  geom_pointrange(aes(y = waic, ymin = waic - se_waic, ymax = waic+ se_waic)) +
  coord_flip() +
  ggthemes::theme_clean() +
  theme(legend.position = "none") +
  labs(title = "WAIC", y = "deviance", x = "model")
```


### Relative shark and absolute deer

```{r}
posterior_samples(b11.4) %>% 
  mutate(proportional_odds = exp(b_b_treatment4 - b_b_treatment2)) %>% 
  mean_qi(proportional_odds)
```

On average the switch from treatment 2 to treatment 4 multiply the odds of pulling
tthe left lever by 92%.

> The risk of focusing on relative effects, such as proportional odds, is that
they aren't enough to tell enough whether a variable is important or not.

See the ovethinking box in section 11.1.2, p. 337.

### Aggregated binomial: Chimpanzees again


```{r}
d.aggr <- d %>%
  group_by(treatment, actor, side, cond) %>%
  summarise(left_pulls = sum(pulled_left)) %>%
  ungroup()
d.aggr
```

```{r}
a_file <- here::here("fits", "b11_06.rds")
b11.6 <- readRDS(file = a_file)
# b11.6 <- brms::brm(
#   data = d.aggr,
#   family = binomial,
#   formula = bf(left_pulls | trials(18) ~ a + b,
#                a ~ 0 + actor, 
#                b ~ 0 + treatment,
#                nl = TRUE),
#   prior =  c(prior(normal(0, 1.5), nlpar = a),
#              prior(normal(0, 0.5), nlpar = b)),
#   cores = detectCores(), seed = 11
#   )
# b11.6 <- brms::add_criterion(b11.6, criterion = c("waic", "loo"))
# saveRDS(b11.6, file = a_file)
summary(b11.6)
brms::inv_logit_scaled(fixef(b11.6))
```


### Aggregated binomial: Graduate school admissions

In the chimpanzees example, the number of trials was fixed at 18.  This is
often not the case.


```{r}
data(UCBadmit)
d <- UCBadmit %>%
  mutate(gid = factor(applicant.gender, levels = c("male", "female")),
         case = seq_len(nrow(.)))
rm(UCBadmit)
skimr::skim(d)
```


the univariate model is

$$
\begin{align*}
admit_i &\sim \mathcal{Binomial}(n_i, p_i) \\
logit(p_i) &= \alpha_{gid[i]} \\
\alpha_j &\sim \mathcal{N}(0, 1.5)
\end{align*}
$$
and we fit the model

```{r}
a_file <- here::here("fits", "b11_07.rds")
b11.7 <- readRDS(a_file)
# b11.7 <- brm(data = d, 
#       family = binomial,
#       admit | trials(applications) ~ 0 + gid,
#       prior(normal(0, 1.5), class = b),
#       cores = detectCores(), seed = 11)
# b11.7 <- brms::add_criterion(b11.7, criterion = c("waic", "loo"))
# saveRDS(b11.7, file = a_file)
summary(b11.7)
brms::inv_logit_scaled(fixef(b11.7))
```



and the full model is

$$
\begin{align*}
admit_i &\sim \mathcal{Binomial}(n_i, p_i) \\
logit(p_i) &= \alpha_{gid[i]} + \delta_{dept[i]} \\
\alpha_j &\sim \mathcal{N}(0, 1.5) \\
\delta_k &\sim \mathcal{N}(0, 1.5)
\end{align*}
$$
and we fit the model

```{r}
a_file <- here::here("fits", "b11_08.rds")
b11.8 <- readRDS(a_file)
# b11.8 <- brm(data = d, 
#       family = binomial,
#       bf(admit | trials(applications) ~ a + d,
#          a ~ 0 + gid, 
#          d ~ 0 + dept,
#          nl = TRUE),
#       prior = c(prior(normal(0, 1.5), nlpar = a),
#                 prior(normal(0, 1.5), nlpar = d)),
#       iter = 4000, warmup = 1000, cores = detectCores(), seed = 11)
# b11.8 <- brms::add_criterion(b11.8, criterion = c("waic", "loo"))
# saveRDS(b11.8, file = a_file)
summary(b11.8)
brms::inv_logit_scaled(fixef(b11.8))
```


## Poisson regression

$$
\begin{align*}
y_i &\sim \mathcal{Poisson}(\lambda_i) \\
\log{\lambda_i} &= \alpha + \beta (x_i - \bar{x})
\end{align*}
$$


### Example: Oceanic tool complexity

```{r}
data(Kline)
d <- Kline %>%
  mutate(log_pop_s = log(population),
         log_pop_s = as.vector(scale(log_pop_s)),
         cid = factor(contact, levels = c("low", "high")))
rm(Kline)
skimr::skim(d)
```


the model is

$$
total\_tools_i \sim \mathcal{Poisson}(\lambda_i) \\
\log{\lambda_i} = \alpha_{cid[i]} + \beta_{cid[i]} \log{log\_pop\_s_i} \\
\alpha_j \sim \mathcal{N}(0, ?) \\
\beta_k \sim \mathcal{N}(0, ?)
$$


#### Calibrating the priors

> Source: https://ggplot2.tidyverse.org/reference/geom_function.html

For the intercept $\alpha_j$ we try


```{r}
# source: https://ggplot2.tidyverse.org/reference/geom_function.html
dist1 <- c("meanlog" = 0, "sdlog" = 10)
dist2 <- c("meanlog" = 3, "sdlog" = 0.5)
p1 <- ggplot(data = data.frame(x = c(0, 100)), aes(x)) +
  geom_function(fun = dlnorm, n = 201, args = list(meanlog = dist1[1], sdlog = dist1[2]),
                size = 1/2) +
  geom_function(fun = dlnorm, n = 201, args = list(meanlog = dist2[1], sdlog = dist2[2]),
                size = 1/2, color = "blue") +
  annotate(geom = "text", x = 5, y = 0.06,
           size = 3, color = "black", hjust = 0, parse = FALSE,
           label = sprintf("dlnormal(%.1f, %.1f)\n mean = %.1f", 
                           dist1[1], dist1[2], dist1[1] + exp(dist1[2]^2)/2)) +
  annotate(geom = "text", x = 35, y = 0.03,
           size = 3, color = "blue", hjust = 0, parse = FALSE,
           label = sprintf("dlnormal(%.1f, %.1f)\n mean = %.1f", 
                           dist2[1], dist2[2], dist2[1] + exp(dist2[2]^2)/2)) +
  scale_x_continuous(breaks = scales::breaks_width(width = 20)) +
  scale_y_continuous(breaks = scales::breaks_width(width = 0.01)) +
  coord_cartesian(xlim = c(0, 100), ylim = c(0, 0.08)) +
  ggthemes::theme_clean() +
  labs(title = "Prior distributions",
       x = expression("mean number of tools" ~ lambda),
       y = "denisty")
p1
```



For the intercept $\beta_k$ we try

```{r}
set.seed(11)

# how many lines would you like?
n <- 100

# simulate and wrangle
df <- tibble(i = seq_len(n),
       a = rnorm(n, mean = 3, sd = 0.5)) %>% 
  mutate(`beta%~%Normal(0*', '*10)`  = rnorm(n, mean = 0 , sd = 10),
         `beta%~%Normal(0*', '*0.2)` = rnorm(n, mean = 0 , sd = 0.2)) %>% 
  pivot_longer(contains("beta"),
               values_to = "b",
               names_to = "prior") %>% 
  expand(nesting(i, a, b, prior),
         x = seq(from = -2, to = 2, length.out = 100))
  
  # plot
ggplot(df, aes(x = x, y = exp(a + b * x), group = i, color = i)) +
  geom_line(size = 1/4, alpha = 2/3) +
  scale_color_paletteer_c(palette = "pals::kovesi.linear_ternary_green_0_46_c42") + 
  coord_cartesian(ylim = c(0, 100)) +
  ggthemes::theme_hc() +
  theme(legend.position = "none") +
  labs(x = "log population (std)", y = "total tools") +
  facet_wrap(~ prior, labeller = label_parsed)
```

#### Model and fit


The model with the priors as explained just above is

$$
total\_tools_i \sim \mathcal{Poisson}(\lambda_i) \\
\log{\lambda_i} = \alpha_{cid[i]} + \beta_{cid[i]} \log{log\_pop\_s_i} \\
\alpha_j \sim \mathcal{N}(3, 0.5) \\
\beta_k \sim \mathcal{N}(0, 0.2)
$$

The fit with intercept only

```{r}
a_file <- here::here("fits", "b11_09.rds")
b11.9 <- readRDS(a_file)
# b11.9 <- brm(data = d,
#              family = poisson,
#              formula = total_tools ~ 1,
#              prior = c(prior(normal(3, 0.5), class = Intercept)),
#              cores = detectCores(), seed = 11)
# b11.9 <- brms::add_criterion(b11.9, criterion = c("waic", "loo"))
# saveRDS(b11.9, file = a_file)
summary(b11.9)
```

and the model with the interaction between population and contact

```{r}
a_file <- here::here("fits", "b11_10.rds")
b11.10 <- readRDS(a_file)
# b11.10 <- brm(data = d,
#              family = poisson,
#              formula = bf(total_tools ~ a + b * log_pop_s,
#                           a + b ~ 0 + cid,
#                           nl = TRUE),
#              prior = c(prior(normal(3, 0.5), nlpar = a),
#                        prior(normal(0, 0.2), nlpar = b)),
#              cores = detectCores(), seed = 11)
# b11.10 <- brms::add_criterion(b11.10, criterion = c("waic", "loo"))
# saveRDS(b11.10, file = a_file)
summary(b11.10)
```

and we compare the LOO

```{r}
loo::loo_compare(b11.9, b11.10, criterion = "loo") %>%
  print(simplify = FALSE)
```

with the model weights

```{r}
model_weights(b11.9, b11.10) %>% 
  round(digits = 2)
```
and we look ate the pareto k since a warning was issued by `add_criterion()`
above

```{r}
loo::loo(b11.10) %>% loo::pareto_k_table()
```

and the details are

```{r}
tibble(culture = d$culture,
       k = b11.10$criteria$loo$diagnostics$pareto_k) %>% 
  arrange(desc(k)) %>% 
  mutate(across(.cols = where(is.double), .fns = round, digits = 2))
```
**which shows that Hawai is the outlier and is very influential**.

#### Plotting the posterior

```{r}
b11.10_seq <-
  distinct(d, cid) %>% 
  expand(cid, 
         log_pop_s = seq(from = -4.5, to = 2.5, length.out = 100))
b11.10_fitted <- 
  fitted(b11.10,
         newdata = b11.10_seq,
         probs = c(.055, .945)) %>%
  data.frame() %>%
  bind_cols(b11.10_seq)
```



```{r}
cultures <- c("Hawaii", "Tonga", "Trobriand", "Yap")

p1 <- ggplot(b11.10_fitted, aes(x = log_pop_s, group = cid, color = cid)) +
  geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid),
              stat = "identity",
              alpha = 1/4, size = 1/2) +
  geom_point(data = bind_cols(d, b11.10$criteria$loo$diagnostics),
             aes(y = total_tools, size = pareto_k),
             alpha = 4/5) +
  ggrepel::geom_text_repel(data =
                    bind_cols(d, b11.10$criteria$loo$diagnostics) %>%
                    filter(culture %in% cultures) %>%
                    mutate(label = paste(culture, "(", round(pareto_k, digits = 2), ")")),
                  aes(y = total_tools, label = label),
                  size = 3, seed = 11, color = "black", family = "Times") +
  coord_cartesian(xlim = range(b11.10$data$log_pop_s),
                  ylim = c(0, 80)) +
  ggthemes::theme_clean() +
  theme(legend.position = "none") +
  labs(x = "log population (std)",
       y = "total tools")
# p1
```

```{r}
p2 <-
  b11.10_fitted %>%
  mutate(population = exp((log_pop_s * sd(log(d$population))) + mean(log(d$population)))) %>% 

  ggplot(aes(x = population, group = cid, color = cid)) +
  geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid),
              stat = "identity",
              alpha = 1/4, size = 1/2) +
  geom_point(data = bind_cols(d, b11.10$criteria$loo$diagnostics),
             aes(y = total_tools, size = pareto_k),
             alpha = 4/5) +
  scale_x_continuous("population", breaks = c(0, 50000, 150000, 250000)) +
  coord_cartesian(xlim = range(d$population),
                  ylim = c(0, 80)) +
  ggthemes::theme_clean() +
  theme(legend.position = "none") +
  labs(y = "total tools")
# p2
```


and the 2 plots together

```{r}
p1 + p2 +
  plot_annotation(title = "Posterior predictions for Oceanic Tools model",
                  subtitle = "Model b11.10 - Size of points is the paretor_k factor")
```

#### Overthinking: Modeling tool innovation

Using the scientific approach with and ODE (ordinary differential equation)

$$
\Delta T = \alpha P^\beta - \gamma T
$$

which as an equilibrium point at $\Delta T = 0$ and therefore

$$
\hat{T} = \frac{\alpha P^\beta}{\gamma}
$$
with the theorical model **which has no link function**

$$
\begin{align*}
T_i &\sim \mathcal{Poisson}(\lambda_i) \\
\lambda_i &\sim \frac{\alpha P^\beta}{\gamma}
\end{align*}
$$

in practice, the model is modified to exponentiate $\alpha$ to ensure it is
always positive

$$
\begin{align*}
total\_tools_i &\sim \mathcal{Poisson}(\lambda_i) \\
\lambda_i &\sim \exp(\alpha_{cid[i]}) \frac{population_i^{\beta_{cid[i]}}}{\gamma} \\
\alpha_j &\sim \mathcal{N}(1, 1) \\
\beta_j &\sim \mathcal{Exp}(1) \\
\gamma &\sim \mathcal{Exp}(1) \\
\end{align*}
$$

and the fit

```{r}
a_file <- here::here("fits", "b11_11.rds")
b11.11 <- readRDS(a_file)
# b11.11 <- brm(data = d, 
#       family = poisson(link = "identity"),
#       bf(total_tools ~ exp(a) * population^b / g,
#          a + b ~ 0 + cid,
#          g ~ 1,
#          nl = TRUE),
#       prior = c(prior(normal(1, 1), nlpar = a),
#                 prior(exponential(1), nlpar = b, lb = 0),
#                 prior(exponential(1), nlpar = g, lb = 0)),
#       cores = detectCores(), seed = 11,
#       control = list(adapt_delta = .95))
# b11.11 <- brms::add_criterion(b11.11, criterion = c("waic", "loo"))
# saveRDS(b11.11, file = a_file)
summary(b11.11)
```


#### final model comparison

```{r}
loo::loo_compare(b11.9, b11.10, b11.11, criterion = "loo") %>%
  print(simplify = FALSE)
```
So the model b11.11 is slightly better.  Note however that its difference form
model b11.10 is with the standard deviation.

and the model weights.


```{r}
model_weights(b11.9, b11.10, b11.11) %>% 
  round(digits = 2)
```

### Negative binomial (gamma-Poisson) models

This distribution is covered in chapter 12.

> A very comon extension of Poisson GLM is to swap the Poisson distribution
for something called the **Negative Binomial** distribution, also called
**Poisson-Gamma**.  It s a Poisson in disguise because it is a mixture of
differrent Poisson distribution.


### Example: Exposure and the offset

When we have different unit of times, or distance (or other denominator), 
$\tau_i$ for expected number of events $\mu_i$ then 

$$
\lambda = \frac{\mu}{\tau}
$$
and now the link is

$$
\begin{align*}
\log{\lambda_i} &= \log{\frac{\mu_i}{\tau_i}}=\alpha + \beta x_i \\
\log{\lambda_i} &= \log{\mu_i} - log{\tau_i}=\alpha + \beta x_i \\
&\therefore \\
\log{\mu_i} &= log{\tau_i} + \alpha + \beta x_i
\end{align*}
$$

When $\tau_i = 1$ then $\log{\tau_i} = 0$ and we recover the original GLM
link.

#### Example: Monastery with varying $\tau_i$

```{r}
ndays <- 30  # nb of days
ydays <- rpois(ndays, lambda = 1.5)  # nb of manuscripts per day
nweeks <- 4
yweeks <- rpois(nweeks, 0.5*7)  # nb of manuscripts per week
# create the dataframe with all data
d <- data.frame(
  nb = c(ydays, yweeks),
  days = c(rep(1, ndays), rep(7, nweeks)),
  monastery = c(rep(0, ndays), rep(1, nweeks))) %>%
  mutate(days_lg = log(days))
```


the model is

$$
\begin{align*}
nb_i &\sim \mathcal{Poisson}(\mu_i) \\ 
\log{\mu_i} &= log(days_i) + \alpha + \beta \cdot monastery_i \\
\alpha &\sim \mathcal{N}(0, 1) \\
\beta &\sim \mathcal{N}(0, 1) \\
\end{align*}
$$

and the fit. With `brms` you use the `offset()` funciton.

```{r}
a_file <- here::here("fits", "b11_12.rds")
b11.12 <- readRDS(a_file)
# b11.12 <- brm(data = d, 
#       family = poisson,
#       nb ~ 1 + offset(days_lg) + monastery,
#       prior = c(prior(normal(0, 1), class = Intercept),
#                 prior(normal(0, 1), class = b)),
#       cores = detectCores(), seed = 11)
# b11.12 <- brms::add_criterion(b11.12, criterion = c("waic", "loo"))
# saveRDS(b11.12, file = a_file)
summary(b11.12)
```

and to get the rates on the natural scale we use

$$
\begin{align*}
\lambda_{monastery[0]} &= \exp{(\alpha)} \\
\lambda_{monastery[1]} &= \exp{(\alpha + \beta)}
\end{align*}
$$
with the posterior values

```{r}
glimpse(posterior_samples(b11.12))
```



```{r}
posterior_samples(b11.12) %>%
  mutate(lambda_old = exp(b_Intercept),
         lambda_new = exp(b_Intercept + b_monastery)) %>%
  pivot_longer(starts_with("lambda"), names_to = "monastery") %>% 
  mutate(monastery = factor(monastery, levels = c("lambda_old", "lambda_new"))) %>%
  group_by(monastery) %>%
  mean_hdi(value, .width = .89) %>% 
  mutate(across(.cols = where(is.double), .fns = round, digits = 2))
```



## Multinomial and categorical models

**Important**: It is important to read @kurtz2020b in this section because
* McElreath seems to have obtained the wrong results
* Kurtz gives a significantly more details and explanations


$$
\begin{align*}
Pr(y_1, \ldots, y_K \mid n, p_1, \ldots, p_K) &= 
\frac{n!}{\prod_i y_i !} \prod_{i=1}^{K} p_i^{y_i} \\
&=\binom{n}{y_1, \ldots, y_K} \prod_{i=1}^{K} p_i^{y_i}
\end{align*}
$$

and the multinomial logit, called **softmax** is

$$
Pr(k \mid s_1, s_2 \ldots, s_K) = \frac{\exp{(s_k)}}{\sum_{i=1}^{K}\exp{(s_i)}}
$$



### Predictors matched to outcomes


```{r}
# simulate career choices among 500 individuals
n <- 500  # number of individuals
income <- c("1" = 1, "2" = 2, "3" = 5)  # expected income of each career
score  <- 0.5 * income  # scores for each career, based on income


# next line converts scores to probabilities
prob <- rethinking::softmax("1" = score[1], "2" = score[2], "3" = score[3])

set.seed(34302)
# sample chosen career for each individual
career <- sapply(seq_len(n), function(i) sample(1:3, size = 1, prob = prob))
# which gives the following frequencies for career = 1, 2, 3
tabulate(career)
# which is very close to the theoretical softmax from above
round(tabulate(career) / n, 3)
round(prob, 3)
```

and the dataframe is

```{r}
d <- data.frame(
  career = career,
  income = income[as.character(career)])
d %>%
  count(career) %>%
  mutate(pct = 100 * n / sum(n),
         prob = n / sum(n))
# which is very close to the theoretical softmax from above
round(prob, 3)
```



#### Scores

* Scores can be thought of as **weights**.

* Their exact values are not much important as their difference from one another.

For example if you add a constant to the scores from above, you get the same
softmax

```{r}
score_new <- score + 11  # 11 is an arbitrary constant added to the scores
# compute the new softmax
exp(score_new) / sum(exp(score_new))
# which gives the same result and shows that the difference between
# the scores is what matters
rethinking::softmax(score)
```

#### Model of predictors matched to outcomes


$$
\begin{align*}
\overrightarrow{career} &\sim \mathcal{multinomial(career_1, career_2, career_3)} =  \binom{n}{career_1, career_2, career_3} \prod_{i=1}^{3} p_i^{career_i}\\
p_1 &= \frac{\exp{(score_1)}}{\sum_1^3\exp{(score_i)}} \\
p_2 &= \frac{\exp{(score_2)}}{\sum_1^3\exp{(score_i)}} \\
p_3 &= \frac{\exp{(score_3)}}{\sum_1^3\exp{(score_i)}} \\
score_1 &= \alpha_1 + \beta \cdot income_1 \\
score_2 &= \alpha_2 + \beta \cdot income_2 \\
score_3 &= \alpha_3 + \beta \cdot income_3 \\
\alpha_1 &\sim \mathcal{N}(0, 1) \\
\alpha_2 &\sim \mathcal{N}(0, 1) \\
\alpha_3 &\sim \mathcal{N}(0, 1) \\
\beta &\sim \mathcal{N}(0, 0.5) \\
\end{align*}
$$

#### Fit with `stan`

We fit with `stan` using the same code as mcElreath to demonstrate his results
are different


```{r}
# define the model
code_m11.13 <- "
data{
  int N; // number of individuals
  int K; // number of possible careers 
  int career[N]; // outcome
  vector[K] career_income;
}
parameters{
  vector[K - 1] a; // intercepts
  real<lower=0> b; // association of income with choice
}
model{
  vector[K] p;
  vector[K] s;
  a ~ normal(0, 1);
  b ~ normal(0, 0.5);
  s[1] = a[1] + b * career_income[1]; 
  s[2] = a[2] + b * career_income[2]; 
  s[3] = 0; // pivot
  p = softmax(s);
  career ~ categorical(p);
}
"

# wrangle the data
dat_list <- 
  list(N = n, 
       K = 3, 
       career = career, 
       career_income = income)

# fit the model (and save it)
a_file <- here::here("fits", "m11_13.rds")
m11.13 <- readRDS(a_file)
# m11.13 <-
#   rstan::stan(data = dat_list,
#        model_code = code_m11.13,
#        chains = 4)
# saveRDS(m11.13, file = a_file)
```


and check the summary using `rethinking::precis`

Note: although Kurtz results seem to work, they have a high Rhat, just like McElreath
and warnings about divergent points after warmup are issued.  The effective sizes
for Kurtz is much lower than the ones from McElreath

```{r}
# the results are different than McElreath
rethinking::precis(m11.13, depth = 2) %>%
  round(digits = 2)
```
The results are different than McEalreath!


#### Null Model (Intercept-only)

Now modelize step by step to understand what we are doing. As usual we start
we the model with only the intercept.  In the case of multinomial, since every
category is a model in itself, we use an intercept per category.

The 3rd category is the *pivot* and identified as such in the `brm()` function below.
The default of `brm()` is to take the first category as the pivot.

```{r}
a_file <- here::here("fits", "b11_13null.rds")
b11.13null <- readRDS(a_file)
# b11.13null <-
#   brm(data = d, 
#       family = categorical(link = logit, refcat = 3),
#       career ~ 1,
#       prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),
#                 prior(normal(0, 1), class = Intercept, dpar = mu2)),
#       cores = detectCores(), seed = 11)
# b11.13null <- add_criterion(b11.13null, criterion = c("waic", "loo"))
# saveRDS(b11.13null, file = a_file)
summary(b11.13null)
```

It is important to understand the role of the **pivot category**.  it is simple,
the pivot category is used to center the categorical scores.

For example the scores we used so far, when centered with the this category,
are as follows

```{r}
tibble::tibble(
  incomes = income,
  scores = score,
  rescaled_scores = scores - score["3"]
)
```

which is the intercepts we obtain with the null model.  This is an easy check
on the null model.

Now lets see what the fitted values for the $\mu_{cat}$ are. These fitted values
correspond to the **softmax** which is the link function.  Note that,
since each category is separate from the other, `fitted` will return a 3-dimension
array where the 3rd dimension is the category no.

We have 500 rows, just like the original data.  Here we show the result of the first row

```{r}
# No newdata used, so the original data is used by fitted
fitted(b11.13null)[1, , ] %>%
  round(digits = 2) %>%
  t()
```
and we can see that that the multinomial probability is actually very close
to the theoretical softmax


```{r}
tibble::tibble(
  income = income,
  score = score,
  prob = exp(score) / sum(exp(score))
  ) %>%
  round(digits = 2)
```

and using the posterior samples to compute the multinomial probabilities
we do the following

```{r}
posterior_samples(b11.13null) %>% 
  mutate(b_mu3_Intercept = 0) %>% 
  mutate(p1 = exp(b_mu1_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)),
         p2 = exp(b_mu2_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)),
         p3 = exp(b_mu3_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept))) %>% 
  pivot_longer(p1:p3) %>% 
  group_by(name) %>% 
  mean_qi(value) %>% 
  mutate(across(.cols = where(is.double), .fns = round, digits = 2))
```

which match with the theoretical **softmax**.

> This is an important test to make sure we get our model right before going any
further.

#### Full model

> *With `brms` non-linear syntax we can fit the model with one $\beta$
parameter or allow it to vary.  The `lb` argument is used to set the lower bound.

We will create 4 models with varying specs as follows

```{r}
crossing(b  = factor(c("b1 & b2", "b"), levels = c("b1 & b2", "b")),
         lb = factor(c("NA", 0), levels = c("NA", 0))) %>% 
  mutate(fit = paste0("b11.13", letters[1:n()])) %>% 
  relocate(fit)
```

and so the model fits are

```{r}
a_file <- here::here("fits", "b11_13a.rds")
# b11.13a <- readRDS(a_file)
b11.13a <- brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      bf(career ~ 1,
         nlf(mu1 ~ a1 + b1 * 1),
         nlf(mu2 ~ a2 + b2 * 2),
         a1 + a2 + b1 + b2 ~ 1),
      prior = c(prior(normal(0, 1), class = b, nlpar = a1),
                prior(normal(0, 1), class = b, nlpar = a2),
                prior(normal(0, 0.5), class = b, nlpar = b1),
                prior(normal(0, 0.5), class = b, nlpar = b2)),
      cores = detectCores(), seed = 11)
b11.13a <- add_criterion(b11.13a, criterion = c("waic", "loo"))
saveRDS(b11.13a, file = a_file)

a_file <- here::here("fits", "b11_13b.rds")
# b11.13b <- readRDS(a_file)
b11.13b <- brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      bf(career ~ 1,
         nlf(mu1 ~ a1 + b1 * 1),
         nlf(mu2 ~ a2 + b2 * 2),
         a1 + a2 + b1 + b2 ~ 1),
      prior = c(prior(normal(0, 1), class = b, nlpar = a1),
                prior(normal(0, 1), class = b, nlpar = a2),
                prior(normal(0, 0.5), class = b, nlpar = b1, lb = 0),
                prior(normal(0, 0.5), class = b, nlpar = b2, lb = 0)),
      cores = detectCores(), seed = 11,
      control = list(adapt_delta = .99))
b11.13b <- add_criterion(b11.13b, criterion = c("waic", "loo"))
saveRDS(b11.13b, file = a_file)

a_file <- here::here("fits", "b11_13c.rds")
# b11.13c <- readRDS(a_file)
b11.13c <-
  brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      bf(career ~ 1,
         nlf(mu1 ~ a1 + b * 1),
         nlf(mu2 ~ a2 + b * 2),
         a1 + a2 + b ~ 1),
      prior = c(prior(normal(0, 1), class = b, nlpar = a1),
                prior(normal(0, 1), class = b, nlpar = a2),
                prior(normal(0, 0.5), class = b, nlpar = b)),
      cores = detectCores(), seed = 11)
b11.13c <- add_criterion(b11.13c, criterion = c("waic", "loo"))
saveRDS(b11.13c, file = a_file)

a_file <- here::here("fits", "b11_13d.rds")
# b11.13d <- readRDS(a_file)
b11.13d <-
  brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      bf(career ~ 1,
         nlf(mu1 ~ a1 + b * 1),
         nlf(mu2 ~ a2 + b * 2),
         a1 + a2 + b ~ 1),
      prior = c(prior(normal(0, 1), class = b, nlpar = a1),
                prior(normal(0, 1), class = b, nlpar = a2),
                prior(normal(0, 0.5), class = b, nlpar = b, lb = 0)),
      cores = detectCores(), seed = 11,
      control = list(adapt_delta = .99))
b11.13d <- add_criterion(b11.13d, criterion = c("waic", "loo"))
saveRDS(b11.13d, file = a_file)
```

and comparing the performance of the models


```{r}
loo_compare(b11.13null, b11.13a, b11.13b, b11.13c, b11.13d, criterion = "loo") %>%
  print(simplify = FALSE)
```

The results are different than Kurtz, this is caused by the facts that the models
have very similar performance and therefore it doesn't take much to change the ranking.
Most numbers, e.g. looic are similar.

and the model weights

```{r}
model_weights(b11.13null, b11.13a, b11.13b, b11.13c, b11.13d, weights = "loo") %>%
  round(digits = 2)
```


### Predictors matched to observations



### Multinomial in disguise as Poisson



## Summary