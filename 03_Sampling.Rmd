```{r include=FALSE}
library(rethinking)
library(brms)
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
# ggplot2 is loaded by tidybayes or ggdist
# library(tidybayes, quietly = TRUE)
library(ggdist, quietly = TRUE)
library(paletteer, quietly = TRUE)
```

# Sampling the Imaginary

## Sampling from grid-approximation posterior

We use the example from chapter 2.

The grid of $p$ values has a grid size of $grod_size$.

The prior is uniformly  distributed and so, as discussed in Overthinking box of section 2.3.3, p. 35, $P(p)=\frac{1}{1-0}=1$.

```{r}
grid_size <- 1000
d <- tibble(
 p_grid = seq(from = 0, to = 1, length.out = grid_size),
 prior = rep(1, times = grid_size)
)

```


and we calculate the posterior using the data. We compute the likelihood
using the grid of priors from above, then compute the average likelihood
which is the sum of the likelihood.

The posterior is defined in detailed in section 2.3.4, p. 37.

$$
\text{Posterior} = 
 \frac{\text{Likelihood} \times \text{Prior}}
  {\text{Average Likelihood}}
$$

Note the code `stopifnot(sum(d$posterior) == 1)`, it is always a good idea
to verify this . . . you could be surprised how often you miss the mark.

```{r}
# the data, see page 28
data <- c("W","L","W","W","W","L","W","L","W")
n_success <- sum(data == "W")
n_trials <- length(data)

# compute the likelihood each value in the grid
d <- d %>%
    mutate(
        likelihood = dbinom(x = n_success, size = n_trials, prob = d$p_grid),
        posterior = likelihood * prior / sum(likelihood)
    )
str(d)
# verify the posterior
stopifnot(sum(d$posterior) == 1)
```

which gives the estimated posterior probability **of $p$ conditional on the data**
for each point of a grid.

Generate and visualize `n_samples` samples from the grid with the $p$ values
with their respective posterior probability $p$ computed above.

> Note: We use `dplyr::slice_sample` because `dplyr::sample_n` is deprecated.


```{r}
# generate n_sample samples, each sample uses a grid size = grid_size
n_samples <- 1e4
set.seed(3)
the_samples <- d %>%
    slice_sample(n = n_samples, weight_by = posterior, replace = TRUE) %>%
    mutate(id = seq_len(n_samples), .before = p_grid)
sum(the_samples$posterior)
glimpse(the_samples)
```

visualize the sample of water proportion

```{r}
# visualize the samples
cols = paletteer_d(palette="Manu::Kotare")
the_samples$p_grid_dev <- abs(mean(the_samples$p_grid) - the_samples$p_grid)
ggplot(data = the_samples, mapping = aes(x = id, y = p_grid, color = p_grid_dev)) +
  geom_point(size = 0.75, alpha = 0.9) +
  scale_color_gradientn(colors = cols) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = sprintf("%d samples", nrow(the_samples)))
```

visualize the density

```{r}
# show the p density
ggplot(data = the_samples, aes(x = p_grid)) +
    geom_density(aes(y=..scaled..), color = "blue", size = 1, fill = "lightblue") +
    theme_light() +
    labs(title = sprintf("%d samples", nrow(the_samples)))
```


## Sampling to summarize

### Intervals of defined boundaries


```{r}
d %>%
  filter(p_grid < 0.5) %>%
  summarize(sum = sum(posterior))
```


### Intervals of defined mass

We can use ``skimr` as follows

```{r}
intrvl_skim <- skimr::skim_with(
  numeric = skimr::sfl(
    p10 = ~ quantile(., probs = 0.1),
    p80 = ~ quantile(., probs = 0.8),
    p90 = ~ quantile(., probs = 0.9)
    ), 
  append = FALSE)

intrvl_skim(the_samples, p_grid)
quantile(the_samples$posterior, probs=0.9)
the_samples %>%
  skimr::skim()
```



We can also customize `skimr` to obtain the `HPDI` as follows

```{r}
hi_skim <- skimr::skim_with(
    base = skimr::sfl(cnt = ~ n(),
               miss = ~ sum(is.na(.))),
    numeric = skimr::sfl(hpdi = ~ rethinking::HPDI(., prob = 0.89)
                         ),
    append = FALSE
)
hi_skim(the_samples, p_grid)
```

We can get the `HPDI` from rethinking

```{r}
# the HPDI
HPDI(the_samples$p_grid, prob = 0.5)
# the PI
PI(the_samples$p_grid, prob = 0.5)
```
but the favorite method is with `ggdist` which we will use extensively from now
on.

```{r}
ggdist::mean_hdi(.data = the_samples, p_grid, .width = 0.5)
```

```{r}
ggdist::mean_qi(.data = the_samples, p_grid, .width = 0.5)
```

and we can illustrate the intervals with `ggdist` as follows

```{r}
qtl <- c(0.5, 0.8, 0.95, 1)
x_breaks <- ggdist::mean_qi(.data = the_samples$p_grid, .width = qtl) %>%
  select(y, ymin, ymax) %>%
  pivot_longer(cols = c("y", "ymin", "ymax")) %>%
  distinct(value) %>%
  arrange(value) %>%
  round(digits = 2) %>%
  pull()
ggplot(the_samples, aes(x=p_grid)) +
         stat_halfeye(aes(fill=stat(cut_cdf_qi(
           cdf,
           .width = qtl,
           labels = scales::percent_format()
           )))) +
  scale_x_continuous(breaks = x_breaks) +
  scale_fill_paletteer_d(palette = "Manu::Takahe", direction = -1,
                         na.translate = FALSE) +
  theme_ggdist() +
  theme(legend.position = c(0.1, 0.75)) +
  labs(title = "Intervals of defined mass",
       x = "p_grid", y = "prob of p_grid",fill = "quantiles")
```




### Point estimates (loss function)

The **linex loss function** can be very useful in business analysis. This is to be
investigated later.

## Sampling to simulate prediction

## Summary
