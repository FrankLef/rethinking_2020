```{r include=FALSE}
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(tidybayes, quietly = TRUE)
library(rethinking)
library(brms)
library(loo)
library(dagitty, quietly = TRUE)
library(ggdag, quietly = TRUE)
library(ggdist, quietly = TRUE)
library(bayesplot, quietly = TRUE)
library(patchwork, quietly = TRUE)
library(paletteer, quietly = TRUE)
```


# Markov Chain Monte Carlo {#MCMC}

## Good King Markov

We define the algorithm to simulate the King's journey.

```{r}
positions <- integer(1e4)
current <- 10
for (i in seq_len(length(positions))) {
  
  # Step 0: record current position
  positions[i] <- current
  
  # step 1: flip a coin
  set.seed(10 * i)
  coin <- sample(x = c(-1, 1), size = 1)
  # step 2: nominate the proposal island
  #         we use modulo arithmetic to simulate a clock
  #         constant 1 substracted and added to obtain 10 instead of 0
  proposal <- (positions[i] + coin - 1) %% 10 + 1
  # step 3: count shells and stones
  #         count of shells = proposal, count of stone = current
  # step 4: prob of moving
  prob_move <- proposal / positions[i]
  current <- ifelse(runif(1) < prob_move, proposal, current)
}

# the itinerary dataframe
itinerary <- data.frame(
  week = seq_along(positions),
  island = factor(positions, levels = 1:10, ordered = TRUE)
)
```


and we plot the trajectory for a sample of 100 weeks

```{r}
set.seed(9)
dp <- itinerary %>%
  arrange(week) %>%
  slice_head(n = 250)
p1 <- ggplot(data = dp, aes(x = week, y = island)) +
  geom_point(aes(color = island), size = 1) +
  scale_color_paletteer_d(palette = "ggsci::category10_d3") +
  # scale_color_manual(values = colr) +
  ggthemes::theme_igray() +
  theme(legend.position = "none") +
  labs(subtitle = sprintf("Itinerary for the first %d weeks", nrow(dp)),
       x = "week #", y = "island")

```


and the frequency of visits to each island

```{r}
p2 <- ggplot(data = itinerary, aes(x = island)) +
  geom_bar(aes(fill = island), stat = "count") +
  scale_fill_paletteer_d(palette = "ggsci::category10_d3") +
  # scale_color_manual(values = colr) +
  ggthemes::theme_igray() +
  theme(legend.position = "none") +
  labs(subtitle = sprintf("all %d weeks", nrow(itinerary)),
       x = "island", y = "nb of weeks")
```


which gives figure 9.2

```{r}
p1 + p2 + 
  plot_annotation(title = "Figure 9.2", subtitle = "Metropolis algorithm")
```

## Metropolis algorithms

### Gibbs sampling

It is a variant of the Metropolis-Hasting algorithm that is more efficient
and uses pairs of conjugate prior and likelihood distributions..

It is the basis for the sofware `BUGS` (Bayesian inference using Gibbs Sampling)
and `JAGS` (Just Another Gibbs Sampler)


### High-dimensional problems

The code for this section comes straight from the same section in @kurtz2020b.
Many thnaks to Solomon Kurtz for this wonderful gift.

The core issue with high-demensional problems is that parameters will end up
having high-correllations which causes the algorithm to get stuck.

McElreath explains it by fire explaining the problem of high correlations itself
then how high-demnsionality leads unavoidebly to high corrrelations.


#### The problem of high correlations

To illustrate a bivariate distribution with strong negative autocorrelaiton
of -0.9 is used

$$
\begin{align*}
\begin{bmatrix}
a_1 \\
a_2
\end{bmatrix}
&\sim
\mathcal{MVNormal}(
\begin{bmatrix}
0 \\
0
\end{bmatrix},
\Sigma
) \\
\Sigma &= \mathbf{SRS} \\
\mathbf{S} &= 
\begin{bmatrix}
0.22 & 0 \\
0 & 0.22
\end{bmatrix} \\
\mathbf{R} &= 
\begin{bmatrix}
1 & -0.9 \\
-0.9 & 1
\end{bmatrix}
\end{align*}
$$
we create the contour of $x$ and $y$ values and their bivariate density.
See https://stackoverflow.com/questions/36221596/plot-multivariate-gaussian-contours-with-ggplot2
for reference.

```{r}
# Create the Multivariate distribution matrix
mu <- c(0, 0)
sd_a1 <- 0.22
sd_a2 <- 0.22
rho <- -0.9
S <- matrix(c(sd_a1, 0, 0, sd_a2), nrow = 2, byrow = TRUE)
R <- matrix(c(1, rho, rho, 1), nrow = 2, byrow = TRUE)
Sigma <- S %*% R %*% S
```

then create the basic contour map

```{r}
x_domain <- seq(from = -1.6, to = 1.6, length.out = 200)
y_domain <- seq(from = -1.6, to = 1.6, length.out = 200)
df <- tidyr::expand_grid(a1 = x_domain, a2 = y_domain)
df <- df %>%
  mutate(prob = mvtnorm::dmvnorm(x = as.matrix(df), mean = mu, sigma = Sigma))
pcontour <- ggplot(df, aes(x = a1, y = a2, z = prob)) + 
  geom_contour(aes(color = ..level..),breaks = 9^(-(10 * 1:25))) +
  scale_color_paletteer_c("grDevices::Emrld", direction = -1) +
  theme_minimal()
pcontour
```
Define a funciton to implement the Metropolis algorithm.  This is a copy
from the same section in kurtz2020b.

```{r}
metropolis <- function(mu, Sigma, num_proposals,
                       step_size,
                       starting_point) {
  
  # Initialize vectors where we will keep track of relevant
  candidate_x_history <- rep(-Inf, num_proposals)
  candidate_y_history <- rep(-Inf, num_proposals)
  did_move_history <- rep(FALSE, num_proposals)
  
  # Prepare to begin the algorithm...
  current_point <- starting_point
  
  for(i in 1:num_proposals) {
    
    # "Proposals are generated by adding random Gaussian noise
    # to each parameter"
    
    noise <- rnorm(n = 2, mean = 0, sd = step_size)
    candidate_point <- current_point + noise
    
    # store coordinates of the proposal point
    candidate_x_history[i] <- candidate_point[1]
    candidate_y_history[i] <- candidate_point[2]
    
    # evaluate the density of our posterior at the proposal point
    candidate_prob <- mvtnorm::dmvnorm(candidate_point, mean = mu, sigma = Sigma)
    
    # evaluate the density of our posterior at the current point
    current_prob <- mvtnorm::dmvnorm(current_point, mean = mu, sigma = Sigma)
    
    # Decide whether or not we should move to the candidate point
    acceptance_ratio <- candidate_prob / current_prob
    should_move <- ifelse(runif(n = 1) < acceptance_ratio, TRUE, FALSE)
    
    # Keep track of the decision
    did_move_history[i] <- should_move
    
    # Move if necessary
    if(should_move) {
      current_point <- candidate_point
    }
  }
  
  # once the loop is complete, store the relevant results in a tibble
  results <- tibble::tibble(
    candidate_x = candidate_x_history,
    candidate_y = candidate_y_history,
    accept = did_move_history
  )
  
  # compute the "acceptance rate" by dividing the total number of "moves"
  # by the total number of proposals
  
  number_of_moves <- results %>% dplyr::pull(accept) %>% sum(.)
  acceptance_rate <- number_of_moves/num_proposals
  
  return(list(results = results, acceptance_rate = acceptance_rate))
  
}
```

and run the algorithm with step size = 0.1

```{r}
set.seed(9)
round_1 <- metropolis(mu = mu, Sigma = Sigma, num_proposals = 50,
                      step_size = 0.1,
                      starting_point = c(-1,1))
# glimpse(round_1)
```


```{r}
p1 <-
  pcontour + 
  geom_point(data = round_1$results,
             aes(x = candidate_x, y = candidate_y, shape = accept, 
                 fill = accept), 
             inherit.aes = FALSE) +
  scale_shape_manual(values = c(21, 21)) +
  scale_fill_manual(values = c("FALSE" = "red", "TRUE" = "green")) +
  theme(legend.position = "none") + 
  labs(title = "Round # 1",
       subtitle = paste("step size 0.1, accept rate", round_1$acceptance_rate),
       x = "a1",
       y = "a2")
# p1
```
and for round # 2


```{r}
set.seed(9)
round_2 <- metropolis(mu = mu, Sigma = Sigma, num_proposals = 50,
                      step_size = 0.25,
                      starting_point = c(-1,1))
# glimpse(round_2)
```


```{r}
p2 <-
  pcontour + 
  geom_point(data = round_2$results,
             aes(x = candidate_x, y = candidate_y, shape = accept, 
                 fill = accept), 
             inherit.aes = FALSE) +
  scale_shape_manual(values = c(21, 21)) +
  scale_fill_manual(values = c("FALSE" = "red", "TRUE" = "green")) +
  theme(legend.position = "none") + 
  labs(title = "Round # 2",
       subtitle = paste("step size 0.25, accept rate", round_2$acceptance_rate),
       x = "a1",
       y = "a2")
# p2
```


```{r}
p1 + p2 + 
  plot_annotation("Metropolis chain under high correlation")
```

#### Concentration of measure

To do on a rainy day



## Hamiltonian Monte Carlo


To do.  Not critical to do the rest of the book.

## Easy HMC: `ulam` with `brms::brm()`

Same data as in chapter 8.

```{r}
data(rugged)
d <- rugged %>%
  filter(complete.cases(rgdppc_2000)) %>%
  mutate(log_gdp = log(rgdppc_2000),
         is_africa = if_else(cont_africa == 1, "Africa", "Not Africa"),
         is_africa = as.factor(is_africa))
rm(rugged)
dd <- d %>%
  drop_na(rgdppc_2000) %>%
  mutate(log_gdp_s = log_gdp / mean(log_gdp),
         rugged_s = scales::rescale(rugged),
         # rugged_s = rugged / max(rugged),
         rugged_sc = as.vector(scale(rugged_s, center = TRUE, scale = FALSE)),
         cid = as.factor(if_else(cont_africa == 1, "1", "2")))
# skimr::skim(dd)
```


### Preparation


```{r}
dat_slim <- dd %>%
  select(log_gdp_s, rugged_s, rugged_sc, cid) %>%
  list()
str(dat_slim)
```


### Sampling from the posterior

```{r}
mean(dd$rugged_s)
```



```{r}
a_file <- here::here(getwd(), "fits", "b09_01.rds")
b9.1 <- readRDS(file = a_file)
# b9.1 <- brms::brm(data = dd,
#                   family = gaussian,
#                   formula = bf(log_gdp_s ~ 0 + a + b*(rugged_sc),
#                                a ~ 0 + cid,
#                                b ~ 0 + cid,
#                                nl = TRUE),
#                   prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a),
#                             prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a),
#                             prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b),
#                             prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b),
#                             prior(exponential(1), class = sigma)),
#                   chains = 1, cores = 1, seed = 9)
# saveRDS(b9.1, file = a_file)
print(b9.1)
```



## Care and feeding of your Markov chain




## Summary
